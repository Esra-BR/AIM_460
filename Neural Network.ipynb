{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b95ddf20-046d-4b8a-bfae-0d46f1d7ac6d",
   "metadata": {},
   "source": [
    "# Neural Networks: A Technical Deep Dive for Students\n",
    "\n",
    "## Table of Contents\n",
    "1. [Fundamentals and Mathematical Foundation](#fundamentals-and-mathematical-foundation)\n",
    "2. [Architecture Variations](#architecture-variations)\n",
    "3. [Implementation from Scratch](#implementation-from-scratch)\n",
    "4. [Real-World Performance Analysis](#real-world-performance-analysis)\n",
    "5. [Advanced Architectures](#advanced-architectures)\n",
    "6. [Optimization and Regularization](#optimization-and-regularization)\n",
    "7. [Comparative Analysis](#comparative-analysis)\n",
    "8. [Production Considerations](#production-considerations)\n",
    "\n",
    "## Fundamentals and Mathematical Foundation\n",
    "\n",
    "### The Neuron: Building Block of Intelligence\n",
    "\n",
    "A neural network is a computational model inspired by biological neural networks. Each artificial neuron performs a weighted sum of inputs followed by a non-linear activation function.\n",
    "\n",
    "**Mathematical Representation:**\n",
    "```\n",
    "z = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b\n",
    "a = œÉ(z)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `z` = linear combination (pre-activation)\n",
    "- `w` = weights (learnable parameters)\n",
    "- `x` = inputs\n",
    "- `b` = bias term\n",
    "- `œÉ` = activation function\n",
    "- `a` = neuron output (post-activation)\n",
    "\n",
    "### Activation Functions: The Non-Linear Magic\n",
    "\n",
    "**1. Sigmoid (Logistic)**\n",
    "```\n",
    "œÉ(z) = 1 / (1 + e^(-z))\n",
    "```\n",
    "- **Range**: (0, 1)\n",
    "- **Best for**: Binary classification output layers\n",
    "- **Problems**: Vanishing gradients, not zero-centered\n",
    "- **When to use**: Output layer for binary classification\n",
    "\n",
    "**2. Hyperbolic Tangent (tanh)**\n",
    "```\n",
    "tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))\n",
    "```\n",
    "- **Range**: (-1, 1)\n",
    "- **Best for**: Hidden layers in shallow networks\n",
    "- **Advantages**: Zero-centered, stronger gradients than sigmoid\n",
    "- **Problems**: Still suffers from vanishing gradients\n",
    "\n",
    "**3. Rectified Linear Unit (ReLU)**\n",
    "```\n",
    "ReLU(z) = max(0, z)\n",
    "```\n",
    "- **Range**: [0, ‚àû)\n",
    "- **Best for**: Hidden layers in deep networks\n",
    "- **Advantages**: Computationally efficient, mitigates vanishing gradients\n",
    "- **Problems**: Dying ReLU problem (neurons can get stuck at 0)\n",
    "\n",
    "**4. Leaky ReLU**\n",
    "```\n",
    "LeakyReLU(z) = max(Œ±z, z) where Œ± = 0.01\n",
    "```\n",
    "- **Range**: (-‚àû, ‚àû)\n",
    "- **Best for**: Deep networks with dead neuron problems\n",
    "- **Advantages**: Prevents dying ReLU, allows negative activations\n",
    "\n",
    "**5. Softmax (for multi-class)**\n",
    "```\n",
    "softmax(z·µ¢) = e^(z·µ¢) / Œ£‚±º e^(z‚±º)\n",
    "```\n",
    "- **Range**: (0, 1) with Œ£ = 1\n",
    "- **Best for**: Multi-class classification output\n",
    "- **Properties**: Converts logits to probability distribution\n",
    "\n",
    "### Loss Functions: Measuring Performance\n",
    "\n",
    "**1. Mean Squared Error (Regression)**\n",
    "```\n",
    "MSE = (1/n) Œ£·µ¢ (y·µ¢ - ≈∑·µ¢)¬≤\n",
    "```\n",
    "\n",
    "**2. Binary Cross-Entropy (Binary Classification)**\n",
    "```\n",
    "BCE = -(1/n) Œ£·µ¢ [y·µ¢ log(≈∑·µ¢) + (1-y·µ¢) log(1-≈∑·µ¢)]\n",
    "```\n",
    "\n",
    "**3. Categorical Cross-Entropy (Multi-class)**\n",
    "```\n",
    "CCE = -(1/n) Œ£·µ¢ Œ£‚±º y·µ¢‚±º log(≈∑·µ¢‚±º)\n",
    "```\n",
    "\n",
    "### Backpropagation: The Learning Algorithm\n",
    "\n",
    "The chain rule of calculus enables gradient computation through the network:\n",
    "\n",
    "```\n",
    "‚àÇL/‚àÇw = ‚àÇL/‚àÇa √ó ‚àÇa/‚àÇz √ó ‚àÇz/‚àÇw\n",
    "```\n",
    "\n",
    "Where gradients flow backward from output to input, updating weights:\n",
    "```\n",
    "w := w - Œ∑ √ó ‚àÇL/‚àÇw\n",
    "```\n",
    "\n",
    "## Architecture Variations\n",
    "\n",
    "## Architecture Complexity Scale\n",
    "\n",
    "**Simple ‚Üí Complex**\n",
    "1. **MLP**: Basic building block, easy to understand\n",
    "2. **CNN**: Adds spatial processing\n",
    "3. **RNN**: Adds memory/sequence processing  \n",
    "4. **Autoencoder**: Adds reconstruction objective\n",
    "5. **Transformer**: Adds attention mechanism\n",
    "6. **GAN**: Adds adversarial training\n",
    "\n",
    "### 1. Feedforward Neural Networks (Multilayer Perceptrons)\n",
    "\n",
    "A Multi‚ÄëLayer Perceptron is a **feed‚Äëforward** network that transforms an input vector $\\mathbf{x}\\in\\mathbb{R}^d$ into an output $\\hat{\\mathbf{y}}\\in\\mathbb{R}^k$. We define hidden activations recursively by\n",
    "\n",
    "$$\n",
    "\\mathbf{h}^{(0)} = \\mathbf{x}, \n",
    "\\quad\n",
    "\\mathbf{h}^{(\\ell)} = \\sigma\\bigl(W^{(\\ell)}\\,\\mathbf{h}^{(\\ell-1)} + b^{(\\ell)}\\bigr),\n",
    "\\quad \\ell=1,\\dots,L-1,\n",
    "$$\n",
    "\n",
    "and then compute\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = W^{(L)}\\,\\mathbf{h}^{(L-1)} + b^{(L)}.\n",
    "$$\n",
    "\n",
    "Here, $W^{(\\ell)}\\in\\mathbb{R}^{n_\\ell\\times n_{\\ell-1}}$ and $b^{(\\ell)}\\in\\mathbb{R}^{n_\\ell}$ are trainable parameters, and $\\sigma(\\cdot)$ is an element‚Äëwise activation (e.g., ReLU or sigmoid). The universal approximation theorem guarantees that with one sufficiently wide hidden layer and a non‚Äëpolynomial activation, an MLP can approximate any continuous function on a compact domain. In practice, we train by minimizing a supervised loss $\\mathcal{L}(\\hat{\\mathbf{y}},\\mathbf{y})$ via stochastic gradient descent, often with $\\ell_2$ regularization to avoid overfitting. MLPs excel on structured tabular data but lack mechanisms to exploit spatial or temporal structure.\n",
    "\n",
    "\n",
    "**Structure**: Input ‚Üí Hidden Layer(s) ‚Üí Output\n",
    "**Information Flow**: Unidirectional, no cycles\n",
    "**Best For**: \n",
    "- Tabular data classification/regression\n",
    "- Function approximation\n",
    "- Pattern recognition\n",
    "\n",
    "**Advantages**:\n",
    "- Simple to understand and implement\n",
    "- Universal function approximators\n",
    "- Good baseline for many problems\n",
    "\n",
    "**Disadvantages**:\n",
    "- Cannot handle sequential data\n",
    "- No memory of previous inputs\n",
    "- Struggles with high-dimensional raw data (images, text)\n",
    "\n",
    "### 2. Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs process grid‚Äëstructured inputs (e.g., images) by applying local filters that share parameters across spatial locations. A 2D convolutional layer computes feature maps $H^{(\\ell)}\\in\\mathbb{R}^{C_\\ell\\times H_\\ell\\times W_\\ell}$ from $H^{(\\ell-1)}$ via\n",
    "\n",
    "$$\n",
    "[H^{(\\ell)}]_{c,i,j}\n",
    "= \\sigma\\!\\Bigl(\\sum_{c'=1}^{C_{\\ell-1}}\\sum_{u=-k}^{k}\\sum_{v=-k}^{k}\n",
    "W^{(\\ell)}_{c,c',u,v}\\,[H^{(\\ell-1)}]_{c',\\,i+u,\\,j+v}\n",
    "+ b^{(\\ell)}_c\\Bigr),\n",
    "$$\n",
    "\n",
    "where $W^{(\\ell)}_{c,c',u,v}$ are convolution kernels of spatial size $(2k+1)\\times(2k+1)$. Pooling layers (e.g., max‚Äëpooling) downsample spatial dimensions to increase translation invariance. By stacking convolutions and pooling, CNNs learn hierarchical representations: early layers detect edges and textures, while deeper layers capture object parts and semantics. CNNs are the de facto standard for image classification, object detection, and any application involving spatial locality, though they demand large datasets and significant compute resources.\n",
    "\n",
    "\n",
    "\n",
    "**Structure**: Convolution ‚Üí Pooling ‚Üí Fully Connected\n",
    "**Key Components**:\n",
    "- **Convolution layers**: Feature extraction using filters\n",
    "- **Pooling layers**: Dimensionality reduction\n",
    "- **Fully connected**: Final classification\n",
    "\n",
    "**Mathematical Operations**:\n",
    "```\n",
    "Convolution: (f * g)(t) = Œ£‚Çò f(m) √ó g(t-m)\n",
    "Max Pooling: max(x‚ÇÅ, x‚ÇÇ, ..., x‚Çô) over pooling window\n",
    "```\n",
    "\n",
    "**Best For**:\n",
    "- Image classification and computer vision\n",
    "- Spatial pattern recognition\n",
    "- Medical image analysis\n",
    "\n",
    "**Advantages**:\n",
    "- Translation invariance\n",
    "- Parameter sharing (fewer parameters)\n",
    "- Hierarchical feature learning\n",
    "\n",
    "**Disadvantages**:\n",
    "- Not suitable for non-grid data\n",
    "- Computationally intensive\n",
    "- Many hyperparameters to tune\n",
    "\n",
    "### 3. Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs handle sequential data by maintaining a hidden state $\\mathbf{h}_t$ that evolves over time. At each time step $t$, given input $\\mathbf{x}_t\\in\\mathbb{R}^d$, the hidden state updates as\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_t = f\\bigl(W_{xh}\\,\\mathbf{x}_t + W_{hh}\\,\\mathbf{h}_{t-1} + b_h\\bigr),\n",
    "\\quad\n",
    "\\mathbf{y}_t = W_{hy}\\,\\mathbf{h}_t + b_y,\n",
    "$$\n",
    "\n",
    "where $f$ is typically $\\tanh$ or ReLU. Standard RNNs struggle to retain long‚Äëterm information due to vanishing gradients, so advanced variants like **LSTM** introduce gating mechanisms (input, forget, output gates) to control information flow, and **GRU** simplifies this further by combining gates into a single update. RNNs and their gated forms are widely used in language modeling, time‚Äëseries forecasting, and speech recognition, but their sequential nature limits parallelization.\n",
    "\n",
    "\n",
    "\n",
    "**Structure**: Input ‚Üí Hidden State ‚Üí Output (with feedback loop)\n",
    "**Key Feature**: Memory through hidden states\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "```\n",
    "h‚Çú = tanh(W‚Çï‚Çï √ó h‚Çú‚Çã‚ÇÅ + W‚Çì‚Çï √ó x‚Çú + b‚Çï)\n",
    "y‚Çú = W‚Çï·µß √ó h‚Çú + b·µß\n",
    "```\n",
    "\n",
    "**Variants**:\n",
    "\n",
    "**Long Short-Term Memory (LSTM)**:\n",
    "- **Forget Gate**: f‚Çú = œÉ(Wf √ó [h‚Çú‚Çã‚ÇÅ, x‚Çú] + bf)\n",
    "- **Input Gate**: i‚Çú = œÉ(Wi √ó [h‚Çú‚Çã‚ÇÅ, x‚Çú] + bi)\n",
    "- **Output Gate**: o‚Çú = œÉ(Wo √ó [h‚Çú‚Çã‚ÇÅ, x‚Çú] + bo)\n",
    "\n",
    "**Gated Recurrent Unit (GRU)**:\n",
    "- **Reset Gate**: r‚Çú = œÉ(Wr √ó [h‚Çú‚Çã‚ÇÅ, x‚Çú])\n",
    "- **Update Gate**: z‚Çú = œÉ(Wz √ó [h‚Çú‚Çã‚ÇÅ, x‚Çú])\n",
    "\n",
    "**Best For**:\n",
    "- Natural language processing\n",
    "- Time series forecasting\n",
    "- Sequential pattern recognition\n",
    "\n",
    "**Advantages**:\n",
    "- Handles variable-length sequences\n",
    "- Memory of previous inputs\n",
    "- Good for temporal patterns\n",
    "\n",
    "**Disadvantages**:\n",
    "- Vanishing gradient problem (vanilla RNN)\n",
    "- Sequential processing (slow training)\n",
    "- Difficulty with very long sequences\n",
    "\n",
    "### 4. Transformer Networks\n",
    "\n",
    "Transformers replace recurrence with **self‚Äëattention**, enabling each position in an input sequence to attend to all others. Given a sequence matrix $\\mathbf{X}\\in\\mathbb{R}^{T\\times d}$, we compute queries $\\mathbf{Q}$, keys $\\mathbf{K}$, and values $\\mathbf{V}$ via learned linear projections, then perform:\n",
    "\n",
    "$$\n",
    "\\mathrm{Attention}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})\n",
    "= \\mathrm{softmax}\\!\\Bigl(\\tfrac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}}\\Bigr)\\,\\mathbf{V}.\n",
    "$$\n",
    "\n",
    "Multiple attention heads run in parallel, followed by position‚Äëwise feed‚Äëforward layers. This architecture captures long‚Äërange dependencies efficiently and allows full parallelism during training. Transformers power state‚Äëof‚Äëthe‚Äëart models like BERT and GPT, excelling in machine translation, summarization, and question answering. However, their quadratic complexity in sequence length makes them computationally intensive for very long inputs.\n",
    "\n",
    "\n",
    "**Structure**: Self-Attention ‚Üí Feed-Forward ‚Üí Layer Normalization\n",
    "**Key Innovation**: Self-attention mechanism\n",
    "\n",
    "**Attention Mechanism**:\n",
    "```\n",
    "Attention(Q,K,V) = softmax(QK^T/‚àöd‚Çñ)V\n",
    "```\n",
    "\n",
    "**Best For**:\n",
    "- Natural language processing\n",
    "- Machine translation\n",
    "- Large-scale language modeling\n",
    "\n",
    "**Advantages**:\n",
    "- Parallel processing (faster than RNNs)\n",
    "- Better long-range dependency modeling\n",
    "- State-of-the-art performance on many NLP tasks\n",
    "\n",
    "**Disadvantages**:\n",
    "- Quadratic complexity with sequence length\n",
    "- Requires large amounts of data\n",
    "- High computational requirements\n",
    "\n",
    "### 5. Autoencoders\n",
    "\n",
    "An autoencoder is an **unsupervised** model that learns a compressed representation of its input $\\mathbf{x}\\in\\mathbb{R}^d$ through an encoder $E$ and reconstructs it with a decoder $D$. Training minimizes the reconstruction loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x},D(E(\\mathbf{x})))\n",
    "= \\bigl\\|\\mathbf{x} - D(E(\\mathbf{x}))\\bigr\\|^2.\n",
    "$$\n",
    "\n",
    "Variants include **Variational Autoencoders (VAEs)**, which impose a latent prior $p(\\mathbf{z})$ and optimize a variational lower bound to enable sampling, and **Denoising Autoencoders**, which learn to recover clean inputs from corrupted versions. Autoencoders are applied to dimensionality reduction, anomaly detection, and unsupervised feature learning but require careful tuning of the latent dimensionality to balance compression and fidelity.\n",
    "\n",
    "\n",
    "**Structure**: Encoder ‚Üí Latent Space ‚Üí Decoder\n",
    "**Purpose**: Unsupervised representation learning\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "```\n",
    "Encoder: z = f(x)\n",
    "Decoder: xÃÇ = g(z)\n",
    "Loss: L(x, xÃÇ) = ||x - xÃÇ||¬≤\n",
    "```\n",
    "\n",
    "**Variants**:\n",
    "- **Variational Autoencoders (VAEs)**: Probabilistic latent space\n",
    "- **Denoising Autoencoders**: Learn robust representations\n",
    "- **Sparse Autoencoders**: Encourage sparse activations\n",
    "\n",
    "**Best For**:\n",
    "- Dimensionality reduction\n",
    "- Anomaly detection\n",
    "- Data compression\n",
    "- Generative modeling\n",
    "\n",
    "### 6. Generative Adversarial Networks (GANs)\n",
    "\n",
    "GANs consist of two models: a generator $G(\\mathbf{z})$ mapping random noise $\\mathbf{z}\\sim p_z$ to the data space, and a discriminator $D(\\mathbf{x})$ estimating the probability that $\\mathbf{x}$ is real. They engage in a minimax game:\n",
    "\n",
    "$$\n",
    "\\min_G \\max_D\\;\n",
    "\\mathbb{E}_{\\mathbf{x}\\sim p_{\\text{data}}}\\bigl[\\log D(\\mathbf{x})\\bigr]\n",
    "\\;+\\;\n",
    "\\mathbb{E}_{\\mathbf{z}\\sim p_z}\\bigl[\\log\\bigl(1 - D(G(\\mathbf{z}))\\bigr)\\bigr].\n",
    "$$\n",
    "\n",
    "Through alternating updates, the generator learns to produce highly realistic samples, while the discriminator becomes a stronger critic. GANs are celebrated for generating high‚Äëfidelity images, data augmentation, and style transfer, but they are notoriously difficult to train due to instability and mode collapse.\n",
    "\n",
    "\n",
    "**Structure**: Generator vs Discriminator (adversarial training)\n",
    "**Objective**: Minimax game between two networks\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "```\n",
    "min_G max_D V(D,G) = E[log D(x)] + E[log(1 - D(G(z)))]\n",
    "```\n",
    "\n",
    "**Best For**:\n",
    "- Image generation\n",
    "- Data augmentation\n",
    "- Style transfer\n",
    "\n",
    "**Advantages**:\n",
    "- Can generate highly realistic data\n",
    "- No explicit density modeling required\n",
    "\n",
    "**Disadvantages**:\n",
    "- Training instability\n",
    "- Mode collapse\n",
    "- Difficult to evaluate\n",
    "\n",
    "\n",
    "| **Data Type** | **Task** | **Best Architecture** | **Why** |\n",
    "|---------------|----------|----------------------|---------|\n",
    "| Tabular (spreadsheet) | Classification/Regression | **MLP** | Simple, effective for structured data |\n",
    "| Images | Recognition/Classification | **CNN** | Designed for spatial relationships |\n",
    "| Text/Sequences | Language tasks | **Transformer** | Best performance, attention mechanism |\n",
    "| Time Series | Forecasting | **RNN/LSTM** | Handles temporal dependencies |\n",
    "| High-dimensional | Compression/Visualization | **Autoencoder** | Learns meaningful representations |\n",
    "| Creative tasks | Generate new data | **GAN** | Creates realistic synthetic data |\n",
    "\n",
    "\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Start simple**: Begin with MLPs for tabular data\n",
    "- **Match architecture to data**: CNNs for images, RNNs for sequences\n",
    "- **Consider your goal**: Classification vs. generation vs. compression\n",
    "- **Balance complexity**: More complex ‚â† always better\n",
    "- **Data matters most**: Good data with simple model often beats complex model with poor data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ef46a-fb77-475c-8130-43f59ba9e3cb",
   "metadata": {},
   "source": [
    "# CNNs: Spatial Processing and Feature Detection\n",
    "\n",
    "## The Core Idea\n",
    "\n",
    "**CNNs are neural networks designed to see patterns in images.** Instead of treating each pixel independently, CNNs understand that nearby pixels are related and work together to form shapes, edges, and objects.\n",
    "\n",
    "## The Problem with Regular Neural Networks on Images\n",
    "\n",
    "### Regular Neural Networks (MLPs)\n",
    "```\n",
    "Input image: 28x28 pixels = 784 individual numbers\n",
    "Network sees: [0.2, 0.8, 0.1, 0.9, 0.3, ...]\n",
    "Problem: No understanding that pixel 100 is next to pixel 101\n",
    "Result: Can't recognize shapes, edges, or spatial relationships\n",
    "```\n",
    "\n",
    "**It's like trying to understand a painting by looking at individual paint molecules under a microscope - you miss the bigger picture!**\n",
    "\n",
    "### What We Actually Need\n",
    "```\n",
    "Network should see:\n",
    "- Groups of pixels forming edges\n",
    "- Edges forming shapes  \n",
    "- Shapes forming objects\n",
    "- Objects in spatial relationships\n",
    "```\n",
    "\n",
    "## Real-World Analogies\n",
    "\n",
    "### 1. Reading a Book vs. Examining Individual Letters\n",
    "**Bad approach (Regular NN)**:\n",
    "- Look at each letter individually: \"T\", \"h\", \"e\", \" \", \"c\", \"a\", \"t\"\n",
    "- Try to understand meaning from isolated letters\n",
    "- Miss words, sentences, paragraphs\n",
    "\n",
    "**Good approach (CNN)**:\n",
    "- First recognize letter combinations ‚Üí words\n",
    "- Then word combinations ‚Üí sentences  \n",
    "- Then sentence combinations ‚Üí paragraphs\n",
    "- Build understanding hierarchically\n",
    "\n",
    "### 2. Medical Diagnosis: X-ray Analysis\n",
    "**How a radiologist reads an X-ray:**\n",
    "1. **First glance**: Notice overall brightness, contrast (low-level features)\n",
    "2. **Closer look**: Identify bones, organs, air spaces (mid-level features)  \n",
    "3. **Expert analysis**: Spot fractures, tumors, abnormalities (high-level features)\n",
    "4. **Diagnosis**: Combine all observations for final conclusion\n",
    "\n",
    "**This is exactly how CNNs work - building from simple to complex features!**\n",
    "\n",
    "### 3. Face Recognition\n",
    "**How you recognize a friend:**\n",
    "1. **Basic features**: Light/dark regions, edges, contrasts\n",
    "2. **Facial features**: Eyes, nose, mouth shapes\n",
    "3. **Face structure**: How features are arranged\n",
    "4. **Recognition**: \"That's Sarah!\"\n",
    "\n",
    "You don't memorize every pixel - you learn hierarchical patterns.\n",
    "\n",
    "## How CNNs Process Images: The Hierarchy\n",
    "\n",
    "### Layer 1: Edge Detection (Low-level Features)\n",
    "**What it detects**: Basic edges and lines\n",
    "\n",
    "```\n",
    "Original image:    Filter detects:        Result:\n",
    "     ‚ñà‚ñà‚ñà‚ñà              Vertical edges  ‚Üí   |  |\n",
    "  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             Horizontal edges    ----\n",
    "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           Diagonal edges      /  \\\n",
    "```\n",
    "\n",
    "**Real example**: In a photo of a cat\n",
    "- Detects edges of whiskers, ears, body outline\n",
    "- Doesn't know it's a \"cat\" yet, just sees lines and curves\n",
    "\n",
    "### Layer 2: Shape Detection (Mid-level Features)  \n",
    "**What it detects**: Combinations of edges forming shapes\n",
    "\n",
    "```\n",
    "Input (edges):     Combines to find:    Result:\n",
    "  |  |               Rectangles         ‚ñ¨‚ñ¨‚ñ¨\n",
    "  |  |      ‚Üí        Circles           ‚¨Æ‚¨Æ‚¨Æ  \n",
    "  ----              Triangles          ‚ñ≤‚ñ≤‚ñ≤\n",
    "```\n",
    "\n",
    "**Real example**: In the cat photo\n",
    "- Combines whisker edges ‚Üí detects \"whisker patterns\"\n",
    "- Combines ear edges ‚Üí detects \"triangular ear shapes\"\n",
    "- Still doesn't know it's a cat, but recognizes cat-like shapes\n",
    "\n",
    "### Layer 3: Object Parts (High-level Features)\n",
    "**What it detects**: Meaningful object components\n",
    "\n",
    "```\n",
    "Input (shapes):    Combines to find:     Result:\n",
    "  ‚¨Æ‚¨Æ + |  |         Eyes + whiskers  ‚Üí   Cat face features\n",
    "  ‚ñ≤‚ñ≤ + curves       Ears + curves        Cat head shape  \n",
    "```\n",
    "\n",
    "**Real example**: \n",
    "- Combines eye shapes + whisker patterns ‚Üí \"cat face\"\n",
    "- Combines ear triangles + fur texture ‚Üí \"cat head\"\n",
    "- Getting closer to understanding \"cat\"!\n",
    "\n",
    "### Layer 4: Full Object Recognition\n",
    "**What it detects**: Complete objects\n",
    "\n",
    "```\n",
    "Input (object parts):  Final recognition:\n",
    "Cat face + cat body  ‚Üí     \"This is a CAT!\"\n",
    "```\n",
    "\n",
    "## The Convolution Operation: How It Works\n",
    "\n",
    "### The Filter/Kernel Concept\n",
    "**Think of a filter as a \"pattern detector\"**\n",
    "\n",
    "```\n",
    "3x3 Edge Detection Filter:\n",
    "[-1  0  1]\n",
    "[-1  0  1]  ‚Üê This pattern detects vertical edges\n",
    "[-1  0  1]\n",
    "```\n",
    "\n",
    "### How Convolution Works: Step by Step\n",
    "\n",
    "#### Step 1: Place Filter on Image\n",
    "```\n",
    "Image patch:       Filter:           Calculation:\n",
    "[100 100 200]  √ó  [-1  0  1]    =  (-100√ó1) + (100√ó0) + (200√ó1)\n",
    "[100 100 200]     [-1  0  1]       + (-100√ó1) + (100√ó0) + (200√ó1)  \n",
    "[100 100 200]     [-1  0  1]       + (-100√ó1) + (100√ó0) + (200√ó1)\n",
    "                                   = 300 (strong vertical edge detected!)\n",
    "```\n",
    "\n",
    "#### Step 2: Slide Filter Across Entire Image\n",
    "```\n",
    "Filter starts here:    Then moves right:    Then continues:\n",
    "[X X X] . . .         . [X X X] . .       . . [X X X] .\n",
    "[X X X] . . .    ‚Üí    . [X X X] . .   ‚Üí   . . [X X X] .\n",
    "[X X X] . . .         . [X X X] . .       . . [X X X] .\n",
    "```\n",
    "\n",
    "**Result**: A new image (feature map) showing where edges were detected!\n",
    "\n",
    "## Different Types of Filters\n",
    "\n",
    "### 1. Edge Detection Filters\n",
    "```\n",
    "Vertical edges:        Horizontal edges:      Diagonal edges:\n",
    "[-1  0  1]            [-1 -1 -1]             [-1  0  1]\n",
    "[-1  0  1]            [ 0  0  0]             [ 0  0  0] \n",
    "[-1  0  1]            [ 1  1  1]             [ 1  0 -1]\n",
    "```\n",
    "\n",
    "### 2. Blur Filters  \n",
    "```\n",
    "Gaussian blur:\n",
    "[1  2  1]\n",
    "[2  4  2]  √∑ 16  (normalize to prevent brightness change)\n",
    "[1  2  1]\n",
    "```\n",
    "\n",
    "### 3. Sharpening Filters\n",
    "```\n",
    "[ 0 -1  0]\n",
    "[-1  5 -1]  (enhances differences between pixels)\n",
    "[ 0 -1  0]\n",
    "```\n",
    "\n",
    "## Pooling: Reducing Size While Keeping Information\n",
    "\n",
    "### Max Pooling: \"What's the strongest signal?\"\n",
    "```\n",
    "Input 4x4:           Max Pool 2x2:        Output 2x2:\n",
    "[1  3  2  4]         Take maximum         [3  4]\n",
    "[2  1  1  3]    ‚Üí    from each 2x2   ‚Üí    [8  9]\n",
    "[5  8  7  9]         region\n",
    "[6  2  3  1]\n",
    "```\n",
    "\n",
    "**Why this works**: If there's an edge detected anywhere in a region, we keep that information but reduce the image size.\n",
    "\n",
    "**Real-world analogy**: Like making a summary of a book chapter - keep the key points, reduce the length.\n",
    "\n",
    "### Average Pooling: \"What's the general signal?\"\n",
    "```\n",
    "Input 4x4:           Avg Pool 2x2:        Output 2x2:\n",
    "[1  3  2  4]         Take average         [1.75  2.5]\n",
    "[2  1  1  3]    ‚Üí    from each 2x2   ‚Üí    [5.25  5.0]\n",
    "[5  8  7  9]         region  \n",
    "[6  2  3  1]\n",
    "```\n",
    "\n",
    "## Complete CNN Architecture Example\n",
    "\n",
    "### Processing a Cat Photo Through CNN Layers\n",
    "\n",
    "```\n",
    "Input: 224x224x3 color image of a cat\n",
    "\n",
    "Layer 1 (Convolution + ReLU):\n",
    "- 32 filters, size 3x3\n",
    "- Detects: edges, lines, basic textures\n",
    "- Output: 222x222x32 (32 different edge maps)\n",
    "\n",
    "Layer 2 (Max Pooling):\n",
    "- Pool size 2x2  \n",
    "- Reduces size by half\n",
    "- Output: 111x111x32\n",
    "\n",
    "Layer 3 (Convolution + ReLU):\n",
    "- 64 filters, size 3x3\n",
    "- Detects: shapes, patterns, curves\n",
    "- Output: 109x109x64\n",
    "\n",
    "Layer 4 (Max Pooling):  \n",
    "- Output: 54x54x64\n",
    "\n",
    "Layer 5 (Convolution + ReLU):\n",
    "- 128 filters, size 3x3\n",
    "- Detects: cat ears, whiskers, eyes\n",
    "- Output: 52x52x128\n",
    "\n",
    "... (more layers) ...\n",
    "\n",
    "Final layers:\n",
    "- Flatten: Convert to 1D vector\n",
    "- Dense: Traditional neural network layers\n",
    "- Output: [0.9 cat, 0.1 dog] ‚Üí \"It's a cat!\"\n",
    "```\n",
    "\n",
    "## Key CNN Concepts Explained\n",
    "\n",
    "### 1. Local Connectivity\n",
    "**Regular NN**: Every pixel connects to every neuron (millions of connections!)\n",
    "**CNN**: Each neuron only looks at a small patch (much fewer connections)\n",
    "\n",
    "```\n",
    "Regular NN:           CNN:\n",
    "Pixel 1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  Neuron sees only:\n",
    "Pixel 2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  [X X X]\n",
    "Pixel 3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  [X X X] ‚Üê 3x3 local patch\n",
    "    ...               [X X X]\n",
    "Pixel 1000000 ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí\n",
    "```\n",
    "\n",
    "### 2. Parameter Sharing\n",
    "**Key insight**: The same edge detector works everywhere in an image!\n",
    "\n",
    "```\n",
    "One 3x3 filter can detect vertical edges:\n",
    "- In top-left corner\n",
    "- In center  \n",
    "- In bottom-right corner\n",
    "- Everywhere!\n",
    "\n",
    "Instead of learning different detectors for each location,\n",
    "use the SAME detector everywhere (much more efficient!)\n",
    "```\n",
    "\n",
    "### 3. Translation Invariance\n",
    "**What this means**: CNN recognizes objects regardless of where they appear in the image\n",
    "\n",
    "```\n",
    "Cat in top-left:     Cat in center:       Cat in bottom-right:\n",
    "  üê±_______           _____üê±_____         _______üê±\n",
    "  ________            ____________         ________\n",
    "  ________            ____________         ________\n",
    "\n",
    "Same filters detect the cat in all positions!\n",
    "```\n",
    "\n",
    "## Receptive Field: How Much Can Each Neuron \"See\"?\n",
    "\n",
    "### Layer-by-Layer Vision Expansion\n",
    "```\n",
    "Layer 1 neuron sees: 3x3 pixels   (tiny local patch)\n",
    "Layer 2 neuron sees: 5x5 pixels   (small neighborhood) \n",
    "Layer 3 neuron sees: 7x7 pixels   (larger area)\n",
    "...\n",
    "Final layer sees: Entire image     (global view)\n",
    "```\n",
    "\n",
    "**Analogy**: Like zooming out with a camera\n",
    "- Start with macro lens (tiny details)\n",
    "- Gradually zoom out to see bigger picture\n",
    "- End with wide-angle view (entire scene)\n",
    "\n",
    "## Why CNNs Work So Well for Images\n",
    "\n",
    "### 1. Hierarchical Learning\n",
    "```\n",
    "Level 1: Pixels       ‚Üí Edges\n",
    "Level 2: Edges        ‚Üí Shapes  \n",
    "Level 3: Shapes       ‚Üí Object parts\n",
    "Level 4: Object parts ‚Üí Objects\n",
    "Level 5: Objects      ‚Üí Scenes\n",
    "```\n",
    "\n",
    "This matches how humans process visual information!\n",
    "\n",
    "### 2. Spatial Locality\n",
    "**Key insight**: Nearby pixels are related, distant pixels usually aren't\n",
    "\n",
    "```\n",
    "In a photo of a face:\n",
    "- Eye pixels relate to nearby eye pixels ‚úì\n",
    "- Eye pixels don't relate to sky pixels in background ‚úó\n",
    "```\n",
    "\n",
    "CNNs focus on local relationships first, then build up to global understanding.\n",
    "\n",
    "### 3. Feature Reusability  \n",
    "**Efficiency**: The same \"eye detector\" works for:\n",
    "- Human eyes\n",
    "- Cat eyes  \n",
    "- Dog eyes\n",
    "- Any circular feature with dark center\n",
    "\n",
    "## Common CNN Architectures\n",
    "\n",
    "### 1. LeNet (1998) - The Pioneer\n",
    "```\n",
    "Input ‚Üí Conv ‚Üí Pool ‚Üí Conv ‚Üí Pool ‚Üí FC ‚Üí FC ‚Üí Output\n",
    "Simple, proved CNNs work for digit recognition\n",
    "```\n",
    "\n",
    "### 2. AlexNet (2012) - The Breakthrough  \n",
    "```\n",
    "Input ‚Üí Conv ‚Üí Pool ‚Üí Conv ‚Üí Pool ‚Üí Conv ‚Üí Conv ‚Üí Conv ‚Üí Pool ‚Üí FC ‚Üí FC ‚Üí FC ‚Üí Output\n",
    "Won ImageNet, started deep learning revolution\n",
    "```\n",
    "\n",
    "### 3. VGGNet (2014) - Deeper Networks\n",
    "```\n",
    "Very deep (16-19 layers), small 3x3 filters throughout\n",
    "Showed that depth matters more than filter size\n",
    "```\n",
    "\n",
    "### 4. ResNet (2015) - Skip Connections\n",
    "```\n",
    "Allows training very deep networks (50-152 layers)\n",
    "Uses \"shortcuts\" to avoid vanishing gradients\n",
    "```\n",
    "\n",
    "## Practical Applications\n",
    "\n",
    "### 1. Medical Imaging\n",
    "```\n",
    "X-ray ‚Üí CNN ‚Üí Disease detection\n",
    "- Layer 1: Detect bone edges, tissue boundaries\n",
    "- Layer 2: Identify organ shapes, abnormal masses  \n",
    "- Layer 3: Recognize disease patterns\n",
    "- Output: \"Pneumonia detected with 92% confidence\"\n",
    "```\n",
    "\n",
    "### 2. Autonomous Vehicles\n",
    "```\n",
    "Camera feed ‚Üí CNN ‚Üí Driving decisions\n",
    "- Layer 1: Detect lane lines, road edges\n",
    "- Layer 2: Identify cars, pedestrians, signs\n",
    "- Layer 3: Understand traffic scenarios  \n",
    "- Output: \"Stop sign ahead, reduce speed\"\n",
    "```\n",
    "\n",
    "### 3. Facial Recognition\n",
    "```\n",
    "Photo ‚Üí CNN ‚Üí Identity verification\n",
    "- Layer 1: Detect facial edges, contrasts\n",
    "- Layer 2: Find eyes, nose, mouth shapes\n",
    "- Layer 3: Combine into facial structure\n",
    "- Output: \"This is John Smith with 97% confidence\"\n",
    "```\n",
    "\n",
    "## Advantages of CNNs\n",
    "\n",
    "1. **Spatial Awareness**: Understands 2D relationships in images\n",
    "2. **Parameter Efficiency**: Shares filters across image (fewer parameters than regular NNs)\n",
    "3. **Translation Invariant**: Recognizes objects anywhere in image  \n",
    "4. **Hierarchical Learning**: Builds complex understanding from simple features\n",
    "5. **Proven Results**: State-of-the-art performance on visual tasks\n",
    "\n",
    "## Limitations of CNNs\n",
    "\n",
    "1. **Rotation Sensitivity**: Struggles with rotated objects (partially solved by data augmentation)\n",
    "2. **Large Dataset Requirements**: Needs lots of training images\n",
    "3. **Computational Cost**: Requires significant processing power\n",
    "4. **Limited to Grid Data**: Works best with images, not arbitrary data structures\n",
    "5. **Fixed Input Size**: Traditional CNNs require same-size inputs\n",
    "\n",
    "## The \"Aha!\" Moment\n",
    "\n",
    "**Regular Neural Network**: Like describing a painting by listing every pixel color\n",
    "- \"Pixel 1 is red, pixel 2 is blue, pixel 3 is green...\"\n",
    "- No understanding of shapes, objects, or composition\n",
    "\n",
    "**CNN**: Like describing a painting by understanding its structure  \n",
    "- \"There are curved lines forming a face\"\n",
    "- \"The eyes are positioned above the nose\"\n",
    "- \"This is a portrait of a person\"\n",
    "\n",
    "**CNNs don't just process images - they understand visual hierarchies and spatial relationships, just like human vision does.**\n",
    "\n",
    "## Modern Evolution\n",
    "\n",
    "```\n",
    "LeNet (1998): Proved concept works\n",
    "AlexNet (2012): Deep learning breakthrough  \n",
    "VGGNet (2014): Deeper is better\n",
    "ResNet (2015): Skip connections enable very deep networks\n",
    "EfficientNet (2019): Optimal architecture scaling\n",
    "Vision Transformers (2020): Attention for images\n",
    "```\n",
    "\n",
    "**Today**: CNNs remain the backbone of computer vision, though Vision Transformers are emerging as strong competitors for some tasks. CNNs are still preferred for many applications due to their efficiency and proven track record."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f34fc86-6d32-4527-a489-07ce88c986a9",
   "metadata": {},
   "source": [
    "# RNN Memory and Sequence Processing: A Clear English Explanation\n",
    "\n",
    "## The Core Idea\n",
    "\n",
    "**RNNs are neural networks with memory.** Unlike regular neural networks that treat each input independently, RNNs remember what they've seen before and use that memory to make better decisions about new inputs.\n",
    "\n",
    "## The Memory Problem in Regular Neural Networks\n",
    "\n",
    "### Regular Neural Networks (MLPs/CNNs)\n",
    "```\n",
    "Input 1: \"I\"     ‚Üí Output: ???\n",
    "Input 2: \"love\"  ‚Üí Output: ???  \n",
    "Input 3: \"cats\"  ‚Üí Output: ???\n",
    "```\n",
    "**Problem**: Each word is processed in isolation. The network has no idea that \"I love cats\" forms a meaningful sentence.\n",
    "\n",
    "### What We Actually Need\n",
    "```\n",
    "Input 1: \"I\"     ‚Üí Remember: someone is speaking\n",
    "Input 2: \"love\"  ‚Üí Remember: someone loves something  \n",
    "Input 3: \"cats\"  ‚Üí Understand: \"I love cats\" (complete thought)\n",
    "```\n",
    "**Solution**: Memory that carries information from previous inputs forward.\n",
    "\n",
    "## Real-World Analogies\n",
    "\n",
    "### 1. Reading a Story\n",
    "When you read: *\"Sarah walked into the dark room. She turned on the light. It was her birthday party!\"*\n",
    "\n",
    "- After \"Sarah\": You remember there's a female character\n",
    "- After \"dark room\": You remember Sarah is somewhere dark\n",
    "- After \"She\": You know \"She\" = Sarah (using memory)\n",
    "- After \"light\": You understand the room is now bright\n",
    "- After \"birthday party\": Everything clicks together!\n",
    "\n",
    "**Your brain maintains a running memory of context - this is exactly what RNNs do.**\n",
    "\n",
    "### 2. Following GPS Directions\n",
    "```\n",
    "Step 1: \"Turn right on Main Street\"     ‚Üí Memory: I'm on Main Street\n",
    "Step 2: \"Continue for 2 miles\"          ‚Üí Memory: On Main St, been driving 1 mile\n",
    "Step 3: \"Turn left at the traffic light\" ‚Üí Memory: Look for traffic light on Main St\n",
    "```\n",
    "\n",
    "Each instruction builds on the previous ones. Without memory, you'd be lost!\n",
    "\n",
    "### 3. Watching a Movie\n",
    "- **Minute 5**: Hero meets villain ‚Üí Memory: These two don't like each other\n",
    "- **Minute 45**: Hero says \"We meet again\" ‚Üí You understand the reference\n",
    "- **Minute 90**: Final confrontation ‚Üí All previous encounters give this meaning\n",
    "\n",
    "**Without memory of previous scenes, the movie wouldn't make sense.**\n",
    "\n",
    "## How RNN Memory Actually Works\n",
    "\n",
    "### The Hidden State: RNN's \"Brain\"\n",
    "The hidden state is like a notebook that the RNN updates with each new input:\n",
    "\n",
    "```\n",
    "Initial state: [empty notebook]\n",
    "Input 1: \"The\"  ‚Üí Hidden state: [article encountered]  \n",
    "Input 2: \"cat\"  ‚Üí Hidden state: [article + subject (cat)]\n",
    "Input 3: \"sat\"  ‚Üí Hidden state: [cat is doing something (sat)]\n",
    "Input 4: \"on\"   ‚Üí Hidden state: [cat sat on something]\n",
    "Input 5: \"mat\"  ‚Üí Hidden state: [complete: cat sat on mat]\n",
    "```\n",
    "\n",
    "### Mathematical Intuition (Simplified)\n",
    "```\n",
    "Step 1: Take current input + previous memory\n",
    "Step 2: Process them together  \n",
    "Step 3: Create new memory + output\n",
    "Step 4: Save new memory for next step\n",
    "```\n",
    "\n",
    "More technically:\n",
    "```\n",
    "new_memory = function(current_input + old_memory)\n",
    "output = function(new_memory)\n",
    "```\n",
    "\n",
    "## Types of RNN Memory\n",
    "\n",
    "### 1. Vanilla RNN - Basic Memory\n",
    "**Analogy**: Like having a small notepad\n",
    "- **Strength**: Simple, fast\n",
    "- **Weakness**: Forgets quickly (vanishing gradient problem)\n",
    "- **Best for**: Short sequences (few words/steps)\n",
    "\n",
    "```\n",
    "Memory capacity: ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ (remembers ~5-10 steps back)\n",
    "Processing speed: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (very fast)\n",
    "```\n",
    "\n",
    "### 2. LSTM - Smart Memory Manager\n",
    "**Analogy**: Like having a smart secretary who decides what to remember/forget\n",
    "\n",
    "**Three Gates (Decision Makers)**:\n",
    "1. **Forget Gate**: \"Should I forget old information?\"\n",
    "2. **Input Gate**: \"Should I remember this new information?\"  \n",
    "3. **Output Gate**: \"What should I focus on right now?\"\n",
    "\n",
    "**Example**: Processing \"I lived in Paris. Now I live in Tokyo.\"\n",
    "- **Forget Gate**: When seeing \"Now\", decides to forget \"Paris\"\n",
    "- **Input Gate**: When seeing \"Tokyo\", decides this is important to remember\n",
    "- **Output Gate**: When asked \"Where do you live?\", focuses on \"Tokyo\"\n",
    "\n",
    "```\n",
    "Memory capacity: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (remembers ~100+ steps back)\n",
    "Processing speed: ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (slower due to complexity)\n",
    "```\n",
    "\n",
    "### 3. GRU - Simplified Smart Memory\n",
    "**Analogy**: Like LSTM's younger sibling - does similar job with fewer parts\n",
    "\n",
    "**Two Gates**:\n",
    "1. **Reset Gate**: \"Should I ignore old memory?\"\n",
    "2. **Update Gate**: \"How much should I update my memory?\"\n",
    "\n",
    "```\n",
    "Memory capacity: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (similar to LSTM)\n",
    "Processing speed: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (faster than LSTM)\n",
    "```\n",
    "\n",
    "## Sequential Processing: Step by Step\n",
    "\n",
    "### Example: Sentiment Analysis of \"I love this movie\"\n",
    "\n",
    "#### Step 1: Process \"I\"\n",
    "```\n",
    "Input: \"I\" \n",
    "Previous memory: [empty]\n",
    "New memory: [someone is speaking]  \n",
    "Output: [neutral sentiment so far]\n",
    "```\n",
    "\n",
    "#### Step 2: Process \"love\"  \n",
    "```\n",
    "Input: \"love\"\n",
    "Previous memory: [someone is speaking]\n",
    "New memory: [someone loves something - positive emotion detected]\n",
    "Output: [positive sentiment emerging]\n",
    "```\n",
    "\n",
    "#### Step 3: Process \"this\"\n",
    "```\n",
    "Input: \"this\"  \n",
    "Previous memory: [someone loves something positive]\n",
    "New memory: [someone loves this specific thing]\n",
    "Output: [positive sentiment continues]\n",
    "```\n",
    "\n",
    "#### Step 4: Process \"movie\"\n",
    "```\n",
    "Input: \"movie\"\n",
    "Previous memory: [someone loves this specific thing] \n",
    "New memory: [someone loves this movie - complete thought]\n",
    "Output: [POSITIVE SENTIMENT] \n",
    "```\n",
    "\n",
    "**Key Point**: Each step builds on all previous steps!\n",
    "\n",
    "## Sequence Generation: RNNs Creating New Content\n",
    "\n",
    "### Example: Text Generation\n",
    "**Seed**: \"The weather today is\"\n",
    "\n",
    "#### Step 1:\n",
    "```\n",
    "Input: \"The weather today is\"  \n",
    "Memory: [talking about current weather]\n",
    "Generate: \"sunny\" (most probable next word)\n",
    "```\n",
    "\n",
    "#### Step 2:  \n",
    "```\n",
    "Input: Previous sentence + \"sunny\"\n",
    "Memory: [weather is sunny today]  \n",
    "Generate: \"and\" (connecting word)\n",
    "```\n",
    "\n",
    "#### Step 3:\n",
    "```\n",
    "Input: Previous + \"and\"\n",
    "Memory: [sunny weather, expecting more description]\n",
    "Generate: \"warm\" (continues weather description)\n",
    "```\n",
    "\n",
    "**Result**: \"The weather today is sunny and warm\"\n",
    "\n",
    "## Memory Challenges and Solutions\n",
    "\n",
    "### The Vanishing Gradient Problem\n",
    "\n",
    "**Problem**: Like playing \"telephone\" game with 100 people\n",
    "- **Early in sequence**: \"The cat is black\"\n",
    "- **After 50 steps**: \"Something about an animal\" \n",
    "- **After 100 steps**: \"There was something...\" (memory fades)\n",
    "\n",
    "**Solution Evolution**:\n",
    "```\n",
    "Vanilla RNN ‚Üí LSTM ‚Üí GRU ‚Üí Transformer (attention)\n",
    "Short memory ‚Üí Long memory ‚Üí Efficient memory ‚Üí Perfect memory\n",
    "```\n",
    "\n",
    "### Memory Visualization\n",
    "\n",
    "**Vanilla RNN Memory**:\n",
    "```\n",
    "Step:  1    2    3    4    5    6    7    8    9   10\n",
    "Info:  ‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà  ‚ñà‚ñà   ‚ñà    ‚ñì    ‚ñí    ‚ñë    .    .    .\n",
    "       Strong ‚Üí Weak ‚Üí Gone\n",
    "```\n",
    "\n",
    "**LSTM Memory**:\n",
    "```  \n",
    "Step:  1    2    3    4    5    6    7    8    9   10\n",
    "Info:  ‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà  ‚ñà‚ñà   ‚ñà‚ñà   ‚ñà    ‚ñà    ‚ñì    ‚ñì\n",
    "       Strong ‚Üí ‚Üí ‚Üí ‚Üí ‚Üí Moderate ‚Üí ‚Üí Weak\n",
    "```\n",
    "\n",
    "## Practical Applications\n",
    "\n",
    "### 1. Language Translation\n",
    "```\n",
    "English input: \"I love cats\"  \n",
    "RNN memory tracks:\n",
    "- Step 1: Subject \"I\" \n",
    "- Step 2: Verb \"love\" (positive emotion)\n",
    "- Step 3: Object \"cats\" (animals)\n",
    "- Output: \"J'aime les chats\" (French)\n",
    "```\n",
    "\n",
    "### 2. Stock Price Prediction  \n",
    "```\n",
    "Day 1: Price $100 ‚Üí Memory: [starting point]\n",
    "Day 2: Price $102 ‚Üí Memory: [upward trend +2]  \n",
    "Day 3: Price $105 ‚Üí Memory: [strong upward trend +2,+3]\n",
    "Day 4: Price $103 ‚Üí Memory: [trend weakening +2,+3,-2]\n",
    "Prediction: Likely to continue declining\n",
    "```\n",
    "\n",
    "### 3. Music Generation\n",
    "```\n",
    "Note 1: C ‚Üí Memory: [key of C major likely]\n",
    "Note 2: E ‚Üí Memory: [C-E chord, major scale confirmed]  \n",
    "Note 3: G ‚Üí Memory: [C major chord complete]\n",
    "Next note: Likely F or A (music theory patterns)\n",
    "```\n",
    "\n",
    "## Key Advantages of RNN Memory\n",
    "\n",
    "1. **Context Awareness**: Understands meaning depends on sequence\n",
    "2. **Variable Length**: Can handle sequences of any length  \n",
    "3. **Pattern Recognition**: Learns temporal patterns\n",
    "4. **Flexible Output**: Can generate sequences or single predictions\n",
    "\n",
    "## Key Limitations\n",
    "\n",
    "1. **Sequential Processing**: Must process one step at a time (slow)\n",
    "2. **Memory Decay**: Forgets distant information (vanilla RNN)\n",
    "3. **Complexity**: Advanced versions (LSTM/GRU) are complex\n",
    "4. **Training Difficulty**: Vanishing/exploding gradients\n",
    "\n",
    "## The \"Aha!\" Moment\n",
    "\n",
    "**Regular Neural Network**: Like reading random words from a bag\n",
    "- \"cat\", \"the\", \"sat\" ‚Üí No understanding\n",
    "\n",
    "**RNN**: Like reading a sentence word by word  \n",
    "- \"the\" ‚Üí \"cat\" ‚Üí \"sat\" ‚Üí \"Complete understanding!\"\n",
    "\n",
    "**RNNs don't just process sequences - they build understanding by remembering and connecting information across time.**\n",
    "\n",
    "## Modern Evolution\n",
    "\n",
    "```\n",
    "RNN (1980s): Basic memory, short sequences\n",
    "LSTM (1997): Smart memory, longer sequences  \n",
    "GRU (2014): Efficient memory, good balance\n",
    "Transformer (2017): Perfect memory, parallel processing\n",
    "```\n",
    "\n",
    "**Today**: RNNs still used for streaming data and real-time applications where you can't see the whole sequence at once, but Transformers dominate most sequence tasks due to better performance and parallel training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e310d9b-1b9f-4b97-92b9-3ba6c26226d8",
   "metadata": {},
   "source": [
    "# Attention Mechanisms\n",
    "\n",
    "## The Core Idea\n",
    "\n",
    "**Attention is asking: \"What should I focus on right now?\"**\n",
    "\n",
    "Instead of treating all information equally, attention mechanisms let the neural network decide which parts of the input are most important for the current task.\n",
    "\n",
    "## Real-World Analogies\n",
    "\n",
    "### 1. Reading a Book\n",
    "When you read the sentence: *\"John went to the store. He bought milk.\"*\n",
    "\n",
    "- Your brain automatically knows \"He\" refers to \"John\" \n",
    "- You **pay attention** to \"John\" when processing \"He\"\n",
    "- You don't give equal weight to every word - \"the\" and \"to\" are less important\n",
    "- **This is exactly what attention does in neural networks**\n",
    "\n",
    "### 2. Cocktail Party Effect\n",
    "You're at a noisy party with many conversations happening:\n",
    "- You can **focus** on one conversation while filtering out others\n",
    "- When someone says your name across the room, your attention **shifts**\n",
    "- You're not processing all sounds equally - you're **selectively attending**\n",
    "\n",
    "### 3. Security Guard with Cameras\n",
    "A security guard monitors 20 camera feeds:\n",
    "- **Without attention**: Tries to watch all 20 screens equally (impossible!)\n",
    "- **With attention**: Focuses on the 2-3 cameras showing unusual activity\n",
    "- The system **highlights** which cameras need attention\n",
    "- Guard makes better decisions by focusing on what matters\n",
    "\n",
    "## How Attention Works in Neural Networks\n",
    "\n",
    "### The Problem Without Attention\n",
    "Traditional neural networks process information like this:\n",
    "```\n",
    "Input: \"The cat sat on the mat\"\n",
    "Network: Processes each word separately, loses context\n",
    "Result: Struggles with relationships between words\n",
    "```\n",
    "\n",
    "### The Solution With Attention\n",
    "```\n",
    "Input: \"The cat sat on the mat\"\n",
    "When processing \"sat\":\n",
    "- Attention looks at ALL words\n",
    "- Decides \"cat\" is very relevant (who sat?)\n",
    "- Decides \"mat\" is relevant (where?)\n",
    "- Decides \"the\" is less relevant\n",
    "- Creates weighted connections: cat (0.8), mat (0.6), the (0.1)\n",
    "```\n",
    "\n",
    "## Three Types of Attention\n",
    "\n",
    "### 1. Self-Attention\n",
    "**What it does**: Each word looks at all other words in the same sentence\n",
    "**Example**: In \"The cat sat on the mat\", when processing \"sat\":\n",
    "- Looks at: \"The\" (not important), \"cat\" (very important), \"on\" (somewhat important), \"the\" (not important), \"mat\" (important)\n",
    "- Creates attention scores: [0.1, 0.8, 0.3, 0.1, 0.6]\n",
    "\n",
    "### 2. Cross-Attention\n",
    "**What it does**: Words in one sentence look at words in another sentence\n",
    "**Example**: Translation from English to French\n",
    "- English: \"I love cats\"\n",
    "- French (being generated): \"J'aime les chats\"\n",
    "- When generating \"chats\", it pays attention to \"cats\" in the English sentence\n",
    "\n",
    "### 3. Multi-Head Attention\n",
    "**What it does**: Multiple attention mechanisms run in parallel\n",
    "**Example**: One \"head\" focuses on grammar, another on meaning, another on relationships\n",
    "- **Head 1**: Focuses on subject-verb relationships (\"cat\" ‚Üí \"sat\")\n",
    "- **Head 2**: Focuses on prepositions (\"sat\" ‚Üí \"on\" ‚Üí \"mat\")\n",
    "- **Head 3**: Focuses on adjectives and nouns\n",
    "\n",
    "## The Math (Simplified)\n",
    "\n",
    "Don't worry about the exact formulas, but here's the intuition:\n",
    "\n",
    "```\n",
    "Step 1: Create three vectors for each word\n",
    "- Query (Q): \"What am I looking for?\"\n",
    "- Key (K): \"What do I represent?\"  \n",
    "- Value (V): \"What information do I contain?\"\n",
    "\n",
    "Step 2: Calculate attention scores\n",
    "- Compare each Query with every Key\n",
    "- Higher similarity = higher attention score\n",
    "\n",
    "Step 3: Apply attention scores\n",
    "- Use scores to weight the Values\n",
    "- Sum up weighted Values to get final output\n",
    "```\n",
    "\n",
    "### Simple Example\n",
    "For the word \"sat\" looking at other words:\n",
    "1. **Query**: \"sat\" asks \"Who is doing the action?\"\n",
    "2. **Keys**: \"The\" (article), \"cat\" (subject), \"on\" (preposition), \"mat\" (object)\n",
    "3. **Matching**: \"sat\" query matches strongly with \"cat\" key\n",
    "4. **Result**: \"sat\" gets most of its information from \"cat\"\n",
    "\n",
    "## Why Attention is Revolutionary\n",
    "\n",
    "### Before Attention (RNNs)\n",
    "```\n",
    "Processing: The ‚Üí cat ‚Üí sat ‚Üí on ‚Üí the ‚Üí mat\n",
    "Problem: By the time we reach \"mat\", we might forget \"cat\"\n",
    "```\n",
    "\n",
    "### With Attention (Transformers)\n",
    "```\n",
    "Processing: Look at ALL words simultaneously\n",
    "Benefit: \"mat\" can directly connect to \"cat\" regardless of distance\n",
    "```\n",
    "\n",
    "## Visual Example: Translation\n",
    "\n",
    "**English**: \"I love my cat very much\"\n",
    "**French**: \"J'aime beaucoup mon chat\"\n",
    "\n",
    "When generating \"chat\" (cat in French):\n",
    "- **High attention** to \"cat\" (0.9)\n",
    "- **Medium attention** to \"my\" (0.4) - helps with grammar\n",
    "- **Low attention** to \"very\" (0.1)\n",
    "- **Very low attention** to \"I\", \"love\", \"much\" (0.0-0.1)\n",
    "\n",
    "The network learns: \"When I need to translate an animal word, pay most attention to animal words in the source language.\"\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "1. **Parallel Processing**: Can look at all inputs simultaneously (faster than RNNs)\n",
    "2. **Long-Range Dependencies**: Connects distant words easily\n",
    "3. **Interpretability**: We can visualize what the model is \"paying attention to\"\n",
    "4. **Flexible**: Works for many tasks beyond language\n",
    "\n",
    "## Common Applications\n",
    "\n",
    "- **Language Translation**: Google Translate\n",
    "- **Text Summarization**: Focusing on key sentences\n",
    "- **Question Answering**: Finding relevant parts of a document\n",
    "- **Image Captioning**: Connecting parts of image to words\n",
    "- **Chatbots**: Understanding context in conversations\n",
    "\n",
    "##  Further\n",
    "\n",
    "**Traditional approach**: Process information in order, hope to remember everything\n",
    "**Attention approach**: \"Let me look at everything first, then decide what's important\"\n",
    "\n",
    "It's like the difference between:\n",
    "- Reading a book page by page (traditional)\n",
    "- Skimming the whole book first, then focusing on relevant chapters (attention)\n",
    "\n",
    "Attention doesn't just process information - it **intelligently selects** what information to focus on, making neural networks much more effective at understanding context and relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc5cee-7b94-40ad-86ab-f83296c489bd",
   "metadata": {},
   "source": [
    "# Autoencoders: Compression and Reconstruction \n",
    "\n",
    "## The Core Idea\n",
    "\n",
    "**Autoencoders are neural networks that learn to compress data and then reconstruct it.** They're like learning the \"essence\" of your data - what's truly important vs. what's just noise or redundancy.\n",
    "\n",
    "## The Fundamental Challenge\n",
    "\n",
    "### The Compression Problem\n",
    "```\n",
    "Original data: [1000 dimensions]\n",
    "‚Üì (compress)\n",
    "Bottleneck: [50 dimensions] ‚Üê Must capture essence in much less space\n",
    "‚Üì (reconstruct)  \n",
    "Reconstructed: [1000 dimensions] ‚Üê Should look like original\n",
    "```\n",
    "\n",
    "**Goal**: Learn the most important features that allow perfect (or near-perfect) reconstruction.\n",
    "\n",
    "## Real-World Analogies\n",
    "\n",
    "### 1. Packing for a Trip\n",
    "**The Scenario**: You need to pack 2 weeks of clothes in a carry-on bag.\n",
    "\n",
    "**Encoder (Packing)**:\n",
    "- Look at all your clothes (original data)\n",
    "- Identify what's essential: underwear, 1 jacket, versatile pants\n",
    "- Pack only the most important items (compressed representation)\n",
    "- Ignore redundant items: 5 similar t-shirts ‚Üí pack 2 versatile ones\n",
    "\n",
    "**Decoder (Unpacking)**:\n",
    "- At destination, unpack your essentials\n",
    "- Use versatile pieces in different combinations\n",
    "- Recreate outfits for different occasions\n",
    "- Goal: Have appropriate clothes for every situation despite packing less\n",
    "\n",
    "**Key Insight**: A good \"autoencoder packer\" learns what's truly essential vs. what's redundant.\n",
    "\n",
    "### 2. Learning to Draw Portraits\n",
    "**Training Phase (Learning Compression)**:\n",
    "- **Input**: Thousands of face photos\n",
    "- **Encoder**: Learn that faces have common structure (2 eyes, 1 nose, 1 mouth in specific positions)\n",
    "- **Bottleneck**: Represent any face with just key parameters (eye shape, nose size, mouth width)\n",
    "- **Decoder**: Learn to draw full faces from these key parameters\n",
    "\n",
    "**Result**: You can now draw any face by just specifying a few key characteristics!\n",
    "\n",
    "### 3. Music Compression (Like MP3)\n",
    "**Original audio**: 50MB of raw sound waves\n",
    "**Encoder**: Identifies which frequencies humans can't hear well\n",
    "**Compressed**: 5MB file with only perceptually important frequencies  \n",
    "**Decoder**: Reconstructs audio that sounds nearly identical to human ears\n",
    "\n",
    "**Autoencoder does similar compression but learns what's important automatically!**\n",
    "\n",
    "## How Autoencoders Work: Architecture\n",
    "\n",
    "### Basic Structure\n",
    "```\n",
    "Input Layer (784 pixels)\n",
    "    ‚Üì\n",
    "Encoder Hidden Layers\n",
    "    ‚Üì (compress)\n",
    "Bottleneck/Latent Space (32 dimensions) ‚Üê The \"essence\"\n",
    "    ‚Üì (expand)  \n",
    "Decoder Hidden Layers\n",
    "    ‚Üì\n",
    "Output Layer (784 pixels)\n",
    "```\n",
    "\n",
    "### The Learning Process\n",
    "1. **Forward Pass**: Input ‚Üí Encoder ‚Üí Bottleneck ‚Üí Decoder ‚Üí Output\n",
    "2. **Loss Calculation**: How different is output from input?\n",
    "3. **Backpropagation**: Adjust weights to minimize reconstruction error\n",
    "4. **Iteration**: Repeat until reconstruction is nearly perfect\n",
    "\n",
    "## Types of Autoencoders\n",
    "\n",
    "### 1. Vanilla Autoencoder - Basic Compression\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Input (784) ‚Üí Hidden (256) ‚Üí Bottleneck (32) ‚Üí Hidden (256) ‚Üí Output (784)\n",
    "```\n",
    "\n",
    "**What it learns**: Basic compression, removes noise and redundancy\n",
    "\n",
    "**Example**: MNIST digit compression\n",
    "- **Input**: 28√ó28 = 784 pixel image of handwritten digit\n",
    "- **Bottleneck**: 32 numbers that capture \"digit essence\"\n",
    "- **Output**: Reconstructed 784 pixel image\n",
    "- **Result**: 32 numbers can recreate recognizable digits!\n",
    "\n",
    "**Real-world analogy**: Learning to describe any digit with just 32 characteristics (curves, lines, positions)\n",
    "\n",
    "### 2. Denoising Autoencoder - Noise Removal\n",
    "\n",
    "**Training Process**:\n",
    "```\n",
    "Clean image ‚Üí Add noise ‚Üí Noisy input ‚Üí Autoencoder ‚Üí Clean output\n",
    "```\n",
    "\n",
    "**What it learns**: To ignore noise and focus on true underlying patterns\n",
    "\n",
    "**Example**: Photo restoration\n",
    "- **Input**: Old damaged photo with scratches, dust, blur\n",
    "- **Training**: Show pairs of (damaged photo, clean photo)\n",
    "- **Learning**: Encoder learns to identify what's damage vs. what's real content\n",
    "- **Result**: Can clean up damaged photos automatically\n",
    "\n",
    "**Real-world analogy**: Like a photo restoration expert who can tell the difference between intentional artistic elements and accidental damage.\n",
    "\n",
    "### 3. Variational Autoencoder (VAE) - Generative Model\n",
    "\n",
    "**Key Innovation**: Instead of learning exact compression, learns probability distributions\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Input ‚Üí Encoder ‚Üí [Mean, Variance] ‚Üí Sample ‚Üí Decoder ‚Üí Output\n",
    "                     ‚Üì\n",
    "                Latent Distribution (Gaussian)\n",
    "```\n",
    "\n",
    "**What makes it special**: Can generate new data by sampling from learned distribution\n",
    "\n",
    "**Example**: Face generation\n",
    "- **Training**: Show thousands of face photos\n",
    "- **Learning**: Encoder learns that \"face space\" follows certain statistical patterns\n",
    "- **Generation**: Sample random point from face space ‚Üí Decoder creates new face\n",
    "- **Result**: Can generate unlimited new realistic faces that never existed!\n",
    "\n",
    "**Real-world analogy**: Like learning the \"recipe space\" for cooking - once you understand the statistical patterns of good recipes, you can create new recipes by sampling from that space.\n",
    "\n",
    "### 4. Sparse Autoencoder - Feature Selection\n",
    "\n",
    "**Key Constraint**: Force most neurons in bottleneck to be inactive (sparse)\n",
    "\n",
    "**Why this helps**: Forces network to learn specialized, meaningful features\n",
    "\n",
    "**Example**: Learning parts of faces\n",
    "- **Regular autoencoder**: Might learn blurry, overlapping features\n",
    "- **Sparse autoencoder**: Learns specific parts (left eye detector, nose detector, mouth detector)\n",
    "- **Result**: More interpretable and often better features\n",
    "\n",
    "**Real-world analogy**: Like learning to describe faces using a limited vocabulary of precise terms rather than vague descriptions.\n",
    "\n",
    "### 5. Contractive Autoencoder - Robust Features\n",
    "\n",
    "**Key Idea**: Make the learned representation insensitive to small input changes\n",
    "\n",
    "**How it works**: Penalizes large gradients in the encoder\n",
    "\n",
    "**Example**: Handwriting recognition\n",
    "- **Problem**: Small changes in pen pressure shouldn't change digit identity\n",
    "- **Solution**: Learn features that are stable under small variations\n",
    "- **Result**: More robust digit recognition\n",
    "\n",
    "**Real-world analogy**: Like learning to recognize your friend's face even with different lighting, angles, or expressions.\n",
    "\n",
    "## The Bottleneck: Where the Magic Happens\n",
    "\n",
    "### Understanding the Latent Space\n",
    "\n",
    "**For MNIST digits**, a 2D bottleneck might learn:\n",
    "- **Dimension 1**: \"Roundness\" (0 for straight digits like 1, high for circular digits like 0)\n",
    "- **Dimension 2**: \"Loops\" (number of enclosed areas in the digit)\n",
    "\n",
    "**Visualization**:\n",
    "```\n",
    "High Roundness, 2 Loops: ‚Üí Digit 8\n",
    "High Roundness, 1 Loop:  ‚Üí Digit 0  \n",
    "Low Roundness, 0 Loops:  ‚Üí Digit 1\n",
    "Medium Roundness, 1 Loop: ‚Üí Digit 6\n",
    "```\n",
    "\n",
    "### Interpolation in Latent Space\n",
    "\n",
    "**Amazing property**: Points between learned representations create meaningful intermediate results!\n",
    "\n",
    "**Example**: Face morphing\n",
    "```\n",
    "Person A encoding: [0.2, 0.8, 0.3, ...]\n",
    "Person B encoding: [0.7, 0.1, 0.9, ...]\n",
    "Midpoint encoding: [0.45, 0.45, 0.6, ...] ‚Üí Face that's half A, half B!\n",
    "```\n",
    "\n",
    "## Practical Applications\n",
    "\n",
    "### 1. Dimensionality Reduction\n",
    "**Problem**: Dataset with 10,000 features is hard to visualize and process\n",
    "**Solution**: Autoencoder compresses to 2-3 dimensions for visualization\n",
    "\n",
    "```\n",
    "Original: Customer data with 10,000 features\n",
    "Autoencoder: Learns that customers fall into ~5 main types\n",
    "Compressed: 2D plot showing customer clusters\n",
    "Business value: Targeted marketing strategies\n",
    "```\n",
    "\n",
    "### 2. Anomaly Detection\n",
    "**Key insight**: Autoencoder learns to reconstruct \"normal\" data well, but struggles with anomalies\n",
    "\n",
    "**Example**: Credit card fraud detection\n",
    "```\n",
    "Training: Normal transactions only\n",
    "Result: Autoencoder reconstructs normal transactions perfectly\n",
    "Testing: Fraudulent transactions reconstruct poorly (high error)\n",
    "Decision: If reconstruction error > threshold ‚Üí Flag as suspicious\n",
    "```\n",
    "\n",
    "**Why this works**: Fraud patterns weren't in training data, so autoencoder can't compress/reconstruct them well.\n",
    "\n",
    "### 3. Data Compression\n",
    "**Traditional compression (ZIP, JPEG)**: Hand-crafted rules about what's important\n",
    "**Autoencoder compression**: Learns what's important from your specific data\n",
    "\n",
    "**Example**: Medical image compression\n",
    "- **JPEG**: Uses generic assumptions about natural images\n",
    "- **Medical autoencoder**: Learns that certain anatomical structures are more important\n",
    "- **Result**: Better compression for medical images specifically\n",
    "\n",
    "### 4. Feature Learning for Other Tasks\n",
    "**Process**:\n",
    "1. Train autoencoder on unlabeled data (unsupervised)\n",
    "2. Use encoder part as feature extractor\n",
    "3. Add classifier on top of encoded features\n",
    "4. Fine-tune on labeled data\n",
    "\n",
    "**Why this helps**: Autoencoder learns meaningful representations without needing labels\n",
    "\n",
    "### 5. Data Generation and Augmentation\n",
    "**Problem**: Not enough training data for machine learning model\n",
    "**Solution**: Train VAE on existing data, generate synthetic examples\n",
    "\n",
    "**Example**: Rare disease diagnosis\n",
    "- **Challenge**: Only 100 examples of rare condition\n",
    "- **Solution**: Train VAE on these 100 examples\n",
    "- **Result**: Generate 1000 synthetic examples that look realistic\n",
    "- **Benefit**: More robust diagnostic model\n",
    "\n",
    "## Training Process: Step by Step\n",
    "\n",
    "### Example: Training on Face Images\n",
    "\n",
    "#### Step 1: Initial State\n",
    "```\n",
    "Input: Face image (64√ó64√ó3 = 12,288 pixels)\n",
    "Encoder: Random weights (terrible compression)\n",
    "Bottleneck: 100 random numbers\n",
    "Decoder: Random weights (terrible reconstruction)  \n",
    "Output: Random noise (looks nothing like input face)\n",
    "```\n",
    "\n",
    "#### Step 2: Learning Begins\n",
    "```\n",
    "Loss = ||Input - Output||¬≤ = Very high (images look completely different)\n",
    "Backpropagation: Adjust all weights to reduce this loss\n",
    "```\n",
    "\n",
    "#### Step 3: Early Learning (after 100 iterations)\n",
    "```\n",
    "Encoder: Starting to detect some basic patterns (edges, colors)\n",
    "Bottleneck: 100 numbers that capture some basic image properties\n",
    "Decoder: Can create blurry, vague face-like shapes\n",
    "Output: Blurry blob that vaguely resembles a face\n",
    "```\n",
    "\n",
    "#### Step 4: Intermediate Learning (after 1000 iterations)  \n",
    "```\n",
    "Encoder: Detecting facial features (eyes, noses, mouths)\n",
    "Bottleneck: 100 numbers encoding facial structure\n",
    "Decoder: Can reconstruct recognizable faces\n",
    "Output: Clearly a face, but missing fine details\n",
    "```\n",
    "\n",
    "#### Step 5: Advanced Learning (after 10,000 iterations)\n",
    "```\n",
    "Encoder: Captures detailed facial characteristics  \n",
    "Bottleneck: 100 numbers that efficiently encode face identity\n",
    "Decoder: Reconstructs high-quality faces\n",
    "Output: Nearly identical to input (successful compression!)\n",
    "```\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "### 1. Reconstruction Error\n",
    "```\n",
    "MSE = Mean((Input - Output)¬≤)\n",
    "Lower = Better compression/reconstruction\n",
    "```\n",
    "\n",
    "### 2. Perceptual Quality\n",
    "- **SSIM (Structural Similarity)**: How similar do images look to humans?\n",
    "- **LPIPS (Learned Perceptual Image Patch Similarity)**: Uses deep networks to measure perceptual similarity\n",
    "\n",
    "### 3. Latent Space Quality (for VAEs)\n",
    "- **Latent space interpolation**: Do intermediate points create meaningful results?\n",
    "- **Latent space arithmetic**: Can you do \"man with glasses\" - \"man\" + \"woman\" = \"woman with glasses\"?\n",
    "\n",
    "## Common Challenges and Solutions\n",
    "\n",
    "### 1. Blurry Reconstructions\n",
    "**Problem**: Traditional autoencoders often produce blurry outputs\n",
    "**Cause**: MSE loss penalizes any deviation from average\n",
    "**Solutions**: \n",
    "- Perceptual loss (compare deep features, not pixels)\n",
    "- Adversarial training (add discriminator to ensure realistic outputs)\n",
    "\n",
    "### 2. Mode Collapse (VAEs)\n",
    "**Problem**: VAE generates limited variety of outputs\n",
    "**Cause**: Network finds a few \"safe\" outputs that work for many inputs\n",
    "**Solutions**:\n",
    "- Œ≤-VAE (balance reconstruction vs. latent regularization)\n",
    "- More complex prior distributions\n",
    "\n",
    "### 3. Posterior Collapse (VAEs)\n",
    "**Problem**: Latent variables become meaningless\n",
    "**Cause**: Decoder ignores latent code, generates from bias terms only\n",
    "**Solutions**:\n",
    "- KL annealing (gradually increase KL penalty)\n",
    "- Skip connections that force decoder to use latent code\n",
    "\n",
    "## Autoencoder vs. Other Methods\n",
    "\n",
    "### Autoencoder vs. PCA\n",
    "```\n",
    "PCA (Principal Component Analysis):\n",
    "- Linear compression only\n",
    "- Fast, guaranteed optimal for linear relationships\n",
    "- Interpretable components\n",
    "\n",
    "Autoencoder:\n",
    "- Non-linear compression (much more powerful)\n",
    "- Slower, no guarantees\n",
    "- Features may be harder to interpret\n",
    "```\n",
    "\n",
    "### Autoencoder vs. GAN\n",
    "```\n",
    "Autoencoder:\n",
    "- Learns compression + reconstruction\n",
    "- Stable training\n",
    "- Good for representation learning\n",
    "\n",
    "GAN:\n",
    "- Learns generation only (no encoding)\n",
    "- Unstable training\n",
    "- Often better generation quality\n",
    "```\n",
    "\n",
    "## Key Advantages\n",
    "\n",
    "1. **Unsupervised Learning**: No labels needed, learns from data structure\n",
    "2. **Flexible Architecture**: Can be adapted for many data types and tasks\n",
    "3. **Feature Learning**: Discovers meaningful representations automatically\n",
    "4. **Data Efficiency**: Can work with relatively small datasets\n",
    "5. **Interpretability**: Latent space often has meaningful structure\n",
    "\n",
    "## Key Limitations\n",
    "\n",
    "1. **Blurry Outputs**: Traditional autoencoders tend to average over possibilities\n",
    "2. **Architecture Sensitivity**: Performance depends heavily on architecture choices\n",
    "3. **Local Minima**: Training can get stuck in suboptimal solutions\n",
    "4. **Limited Generation**: Vanilla autoencoders can't generate truly new data\n",
    "5. **Evaluation Difficulty**: Hard to measure quality of learned representations\n",
    "\n",
    "## The \"Aha!\" Moment\n",
    "\n",
    "**Traditional Data Storage**: Keep every detail, even if redundant\n",
    "- Store every pixel of every image separately\n",
    "- No understanding of underlying patterns\n",
    "\n",
    "**Autoencoder Approach**: Learn the underlying structure\n",
    "- \"I notice that faces always have 2 eyes, 1 nose, 1 mouth\"\n",
    "- \"I can represent any face with just eye shape, nose size, mouth position\"\n",
    "- \"Now I can perfectly recreate faces using much less information!\"\n",
    "\n",
    "**Autoencoders don't just compress data - they learn to understand the fundamental patterns that generate your data.**\n",
    "\n",
    "## Modern Evolution and Variants\n",
    "\n",
    "```\n",
    "Basic Autoencoder (1980s): Simple compression/reconstruction\n",
    "Denoising Autoencoder (2008): Robust feature learning\n",
    "Variational Autoencoder (2013): Probabilistic generation  \n",
    "Œ≤-VAE (2017): Disentangled representations\n",
    "Vector Quantized VAE (2017): Discrete latent spaces\n",
    "Transformer Autoencoders (2020s): Attention for sequences\n",
    "```\n",
    "\n",
    "**Today**: Autoencoders remain fundamental building blocks in many modern architectures, from GPT (which uses autoregressive generation) to diffusion models (which learn to denoise). The core principle of learning compressed representations continues to drive advances in AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2ab3d6c-2d90-4108-9a68-ca75eb9827fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPREHENSIVE NEURAL NETWORKS ANALYSIS\n",
      "================================================================================\n",
      "This analysis compares different neural network architectures\n",
      "and activation functions on real-world datasets with inherent\n",
      "noise and class overlap, showing realistic performance metrics.\n",
      "================================================================================\n",
      "NEURAL NETWORK ARCHITECTURE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Dataset: Breast Cancer Wisconsin (Binary Classification)\n",
      "------------------------------------------------------------\n",
      "Training samples: 455\n",
      "Validation samples: 57\n",
      "Test samples: 57\n",
      "Features: 30\n",
      "Classes: 2 (Malignant: 357, Benign: 212)\n",
      "\n",
      "==================== Shallow Network ====================\n",
      "Architecture: 30 ‚Üí 16 ‚Üí 1\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "================================================================================\n",
      "Prediction time: 0.0010 seconds\n",
      "Predictions per second: 57004\n",
      "\n",
      "Overall Performance Metrics:\n",
      "Accuracy:  0.8947\n",
      "Precision: 0.8947\n",
      "Recall:    0.8947\n",
      "F1-Score:  0.8947\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.86      0.86      0.86        21\n",
      "   Malignant       0.92      0.92      0.92        36\n",
      "\n",
      "    accuracy                           0.89        57\n",
      "   macro avg       0.89      0.89      0.89        57\n",
      "weighted avg       0.89      0.89      0.89        57\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18  3]\n",
      " [ 3 33]]\n",
      "\n",
      "Prediction Confidence Analysis:\n",
      "Mean confidence: 0.8891\n",
      "Std confidence:  0.1461\n",
      "Min confidence:  0.5236\n",
      "Max confidence:  1.0000\n",
      "\n",
      "High confidence (>0.8): 44 samples, accuracy: 1.0000\n",
      "Medium confidence (0.6-0.8): 8 samples, accuracy: 0.3750\n",
      "Low confidence (‚â§0.6): 5 samples, accuracy: 0.8000\n",
      "\n",
      "Training time: 1.31 seconds\n",
      "Model parameters: 513\n",
      "\n",
      "==================== Deep Narrow ====================\n",
      "Architecture: 30 ‚Üí 32 ‚Üí 16 ‚Üí 8 ‚Üí 1\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "================================================================================\n",
      "Prediction time: 0.0000 seconds\n",
      "Predictions per second: >1000000 (very fast)\n",
      "\n",
      "Overall Performance Metrics:\n",
      "Accuracy:  0.9474\n",
      "Precision: 0.9474\n",
      "Recall:    0.9474\n",
      "F1-Score:  0.9471\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.95      0.90      0.93        21\n",
      "   Malignant       0.95      0.97      0.96        36\n",
      "\n",
      "    accuracy                           0.95        57\n",
      "   macro avg       0.95      0.94      0.94        57\n",
      "weighted avg       0.95      0.95      0.95        57\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19  2]\n",
      " [ 1 35]]\n",
      "\n",
      "Prediction Confidence Analysis:\n",
      "Mean confidence: 0.9141\n",
      "Std confidence:  0.1318\n",
      "Min confidence:  0.5114\n",
      "Max confidence:  1.0000\n",
      "\n",
      "High confidence (>0.8): 46 samples, accuracy: 1.0000\n",
      "Medium confidence (0.6-0.8): 9 samples, accuracy: 0.7778\n",
      "Low confidence (‚â§0.6): 2 samples, accuracy: 0.5000\n",
      "\n",
      "Training time: 2.07 seconds\n",
      "Model parameters: 1,665\n",
      "\n",
      "==================== Wide Network ====================\n",
      "Architecture: 30 ‚Üí 128 ‚Üí 64 ‚Üí 1\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "================================================================================\n",
      "Prediction time: 0.0000 seconds\n",
      "Predictions per second: >1000000 (very fast)\n",
      "\n",
      "Overall Performance Metrics:\n",
      "Accuracy:  0.9298\n",
      "Precision: 0.9298\n",
      "Recall:    0.9298\n",
      "F1-Score:  0.9298\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.90      0.90      0.90        21\n",
      "   Malignant       0.94      0.94      0.94        36\n",
      "\n",
      "    accuracy                           0.93        57\n",
      "   macro avg       0.92      0.92      0.92        57\n",
      "weighted avg       0.93      0.93      0.93        57\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19  2]\n",
      " [ 2 34]]\n",
      "\n",
      "Prediction Confidence Analysis:\n",
      "Mean confidence: 0.9120\n",
      "Std confidence:  0.1339\n",
      "Min confidence:  0.5437\n",
      "Max confidence:  1.0000\n",
      "\n",
      "High confidence (>0.8): 47 samples, accuracy: 0.9787\n",
      "Medium confidence (0.6-0.8): 7 samples, accuracy: 0.5714\n",
      "Low confidence (‚â§0.6): 3 samples, accuracy: 1.0000\n",
      "\n",
      "Training time: 2.62 seconds\n",
      "Model parameters: 12,289\n",
      "\n",
      "==================== Very Deep ====================\n",
      "Architecture: 30 ‚Üí 64 ‚Üí 32 ‚Üí 16 ‚Üí 8 ‚Üí 4 ‚Üí 1\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "================================================================================\n",
      "Prediction time: 0.0000 seconds\n",
      "Predictions per second: >1000000 (very fast)\n",
      "\n",
      "Overall Performance Metrics:\n",
      "Accuracy:  0.8947\n",
      "Precision: 0.8947\n",
      "Recall:    0.8947\n",
      "F1-Score:  0.8947\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.86      0.86      0.86        21\n",
      "   Malignant       0.92      0.92      0.92        36\n",
      "\n",
      "    accuracy                           0.89        57\n",
      "   macro avg       0.89      0.89      0.89        57\n",
      "weighted avg       0.89      0.89      0.89        57\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18  3]\n",
      " [ 3 33]]\n",
      "\n",
      "Prediction Confidence Analysis:\n",
      "Mean confidence: 0.9075\n",
      "Std confidence:  0.1234\n",
      "Min confidence:  0.5940\n",
      "Max confidence:  1.0000\n",
      "\n",
      "High confidence (>0.8): 48 samples, accuracy: 0.9792\n",
      "Medium confidence (0.6-0.8): 7 samples, accuracy: 0.4286\n",
      "Low confidence (‚â§0.6): 2 samples, accuracy: 0.5000\n",
      "\n",
      "Training time: 3.10 seconds\n",
      "Model parameters: 4,769\n",
      "\n",
      "================================================================================\n",
      "ARCHITECTURE COMPARISON SUMMARY\n",
      "================================================================================\n",
      "                Parameters Training Time (s) Prediction Time (s) Accuracy  \\\n",
      "Shallow Network        513              1.31              0.0010   0.8947   \n",
      "Deep Narrow           1665              2.07              0.0000   0.9474   \n",
      "Wide Network         12289              2.62              0.0000   0.9298   \n",
      "Very Deep             4769              3.10              0.0000   0.8947   \n",
      "\n",
      "                Precision  Recall F1-Score  \n",
      "Shallow Network    0.8947  0.8947   0.8947  \n",
      "Deep Narrow        0.9474  0.9474   0.9471  \n",
      "Wide Network       0.9298  0.9298   0.9298  \n",
      "Very Deep          0.8947  0.8947   0.8947  \n",
      "\n",
      "Key Findings:\n",
      "Best Accuracy: Deep Narrow (0.9474)\n",
      "Fastest Training: Shallow Network (1.31s)\n",
      "Most Efficient: Shallow Network (513 parameters)\n",
      "\n",
      "================================================================================\n",
      "ACTIVATION FUNCTION COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Dataset: Handwritten Digits (Multi-class Classification)\n",
      "Samples: 1797\n",
      "Features: 64 (8x8 pixel values)\n",
      "Classes: 10 (digits 0-9)\n",
      "\n",
      "==================== SIGMOID ====================\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "================================================================================\n",
      "Prediction time: 0.0024 seconds\n",
      "Predictions per second: 148267\n",
      "\n",
      "Overall Performance Metrics:\n",
      "Accuracy:  0.7694\n",
      "Precision: 0.7781\n",
      "Recall:    0.7694\n",
      "F1-Score:  0.7411\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Digit 0       0.94      0.81      0.87        36\n",
      "     Digit 1       0.64      0.69      0.67        36\n",
      "     Digit 2       0.80      0.91      0.85        35\n",
      "     Digit 3       0.78      0.84      0.81        37\n",
      "     Digit 4       0.69      0.97      0.80        36\n",
      "     Digit 5       0.88      0.95      0.91        37\n",
      "     Digit 6       0.77      0.92      0.84        36\n",
      "     Digit 7       0.74      0.94      0.83        36\n",
      "     Digit 8       0.80      0.11      0.20        35\n",
      "     Digit 9       0.76      0.53      0.62        36\n",
      "\n",
      "    accuracy                           0.77       360\n",
      "   macro avg       0.78      0.77      0.74       360\n",
      "weighted avg       0.78      0.77      0.74       360\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[29  0  0  0  4  0  3  0  0  0]\n",
      " [ 0 25  5  0  4  0  2  0  0  0]\n",
      " [ 0  1 32  2  0  0  0  0  0  0]\n",
      " [ 0  0  1 31  0  0  0  2  0  3]\n",
      " [ 0  0  0  0 35  0  0  1  0  0]\n",
      " [ 0  0  0  0  0 35  0  0  0  2]\n",
      " [ 0  1  0  0  2  0 33  0  0  0]\n",
      " [ 0  0  0  1  1  0  0 34  0  0]\n",
      " [ 1 12  2  6  1  2  3  3  4  1]\n",
      " [ 1  0  0  0  4  3  2  6  1 19]]\n",
      "\n",
      "Prediction Confidence Analysis:\n",
      "Mean confidence: 0.1661\n",
      "Std confidence:  0.0316\n",
      "Min confidence:  0.1122\n",
      "Max confidence:  0.2936\n",
      "Low confidence (‚â§0.6): 360 samples, accuracy: 0.7694\n",
      "Training time: 7.67 seconds\n",
      "\n",
      "==================== TANH ====================\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "================================================================================\n",
      "Prediction time: 0.0020 seconds\n",
      "Predictions per second: 180206\n",
      "\n",
      "Overall Performance Metrics:\n",
      "Accuracy:  0.9694\n",
      "Precision: 0.9704\n",
      "Recall:    0.9694\n",
      "F1-Score:  0.9695\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Digit 0       1.00      0.97      0.99        36\n",
      "     Digit 1       0.89      0.94      0.92        36\n",
      "     Digit 2       0.95      1.00      0.97        35\n",
      "     Digit 3       1.00      0.97      0.99        37\n",
      "     Digit 4       0.97      1.00      0.99        36\n",
      "     Digit 5       1.00      0.97      0.99        37\n",
      "     Digit 6       1.00      0.97      0.99        36\n",
      "     Digit 7       1.00      0.97      0.99        36\n",
      "     Digit 8       0.94      0.89      0.91        35\n",
      "     Digit 9       0.95      1.00      0.97        36\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35  0  1  0  0  0  0  0  0  0]\n",
      " [ 0 34  0  0  0  0  0  0  1  1]\n",
      " [ 0  0 35  0  0  0  0  0  0  0]\n",
      " [ 0  0  1 36  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 36  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 36  0  0  0  1]\n",
      " [ 0  0  0  0  0  0 35  0  1  0]\n",
      " [ 0  0  0  0  1  0  0 35  0  0]\n",
      " [ 0  4  0  0  0  0  0  0 31  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 36]]\n",
      "\n",
      "Prediction Confidence Analysis:\n",
      "Mean confidence: 0.8850\n",
      "Std confidence:  0.1478\n",
      "Min confidence:  0.2861\n",
      "Max confidence:  0.9951\n",
      "\n",
      "High confidence (>0.8): 292 samples, accuracy: 0.9966\n",
      "Medium confidence (0.6-0.8): 40 samples, accuracy: 0.9000\n",
      "Low confidence (‚â§0.6): 28 samples, accuracy: 0.7857\n",
      "Training time: 6.75 seconds\n",
      "\n",
      "==================== RELU ====================\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "================================================================================\n",
      "Prediction time: 0.0010 seconds\n",
      "Predictions per second: 360112\n",
      "\n",
      "Overall Performance Metrics:\n",
      "Accuracy:  0.9639\n",
      "Precision: 0.9655\n",
      "Recall:    0.9639\n",
      "F1-Score:  0.9642\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Digit 0       1.00      0.97      0.99        36\n",
      "     Digit 1       0.85      0.94      0.89        36\n",
      "     Digit 2       0.97      1.00      0.99        35\n",
      "     Digit 3       1.00      0.97      0.99        37\n",
      "     Digit 4       0.97      1.00      0.99        36\n",
      "     Digit 5       1.00      0.97      0.99        37\n",
      "     Digit 6       1.00      0.94      0.97        36\n",
      "     Digit 7       0.97      1.00      0.99        36\n",
      "     Digit 8       0.91      0.89      0.90        35\n",
      "     Digit 9       0.97      0.94      0.96        36\n",
      "\n",
      "    accuracy                           0.96       360\n",
      "   macro avg       0.97      0.96      0.96       360\n",
      "weighted avg       0.97      0.96      0.96       360\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 34  0  0  0  0  0  0  2  0]\n",
      " [ 0  0 35  0  0  0  0  0  0  0]\n",
      " [ 0  0  1 36  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 36  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 36  0  0  0  1]\n",
      " [ 0  1  0  0  0  0 34  0  1  0]\n",
      " [ 0  0  0  0  0  0  0 36  0  0]\n",
      " [ 0  4  0  0  0  0  0  0 31  0]\n",
      " [ 0  1  0  0  0  0  0  1  0 34]]\n",
      "\n",
      "Prediction Confidence Analysis:\n",
      "Mean confidence: 0.9149\n",
      "Std confidence:  0.1348\n",
      "Min confidence:  0.2414\n",
      "Max confidence:  1.0000\n",
      "\n",
      "High confidence (>0.8): 307 samples, accuracy: 0.9935\n",
      "Medium confidence (0.6-0.8): 35 samples, accuracy: 0.8857\n",
      "Low confidence (‚â§0.6): 18 samples, accuracy: 0.6111\n",
      "Training time: 4.96 seconds\n",
      "\n",
      "==================== LEAKY_RELU ====================\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "================================================================================\n",
      "Prediction time: 0.0020 seconds\n",
      "Predictions per second: 179949\n",
      "\n",
      "Overall Performance Metrics:\n",
      "Accuracy:  0.9639\n",
      "Precision: 0.9655\n",
      "Recall:    0.9639\n",
      "F1-Score:  0.9642\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Digit 0       1.00      0.97      0.99        36\n",
      "     Digit 1       0.85      0.94      0.89        36\n",
      "     Digit 2       0.97      1.00      0.99        35\n",
      "     Digit 3       1.00      0.97      0.99        37\n",
      "     Digit 4       0.97      1.00      0.99        36\n",
      "     Digit 5       1.00      0.97      0.99        37\n",
      "     Digit 6       1.00      0.94      0.97        36\n",
      "     Digit 7       0.97      1.00      0.99        36\n",
      "     Digit 8       0.91      0.89      0.90        35\n",
      "     Digit 9       0.97      0.94      0.96        36\n",
      "\n",
      "    accuracy                           0.96       360\n",
      "   macro avg       0.97      0.96      0.96       360\n",
      "weighted avg       0.97      0.96      0.96       360\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 34  0  0  0  0  0  0  2  0]\n",
      " [ 0  0 35  0  0  0  0  0  0  0]\n",
      " [ 0  0  1 36  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 36  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 36  0  0  0  1]\n",
      " [ 0  1  0  0  0  0 34  0  1  0]\n",
      " [ 0  0  0  0  0  0  0 36  0  0]\n",
      " [ 0  4  0  0  0  0  0  0 31  0]\n",
      " [ 0  1  0  0  0  0  0  1  0 34]]\n",
      "\n",
      "Prediction Confidence Analysis:\n",
      "Mean confidence: 0.9151\n",
      "Std confidence:  0.1349\n",
      "Min confidence:  0.2389\n",
      "Max confidence:  1.0000\n",
      "\n",
      "High confidence (>0.8): 308 samples, accuracy: 0.9903\n",
      "Medium confidence (0.6-0.8): 34 samples, accuracy: 0.9118\n",
      "Low confidence (‚â§0.6): 18 samples, accuracy: 0.6111\n",
      "Training time: 5.34 seconds\n",
      "\n",
      "================================================================================\n",
      "ACTIVATION FUNCTION COMPARISON SUMMARY\n",
      "================================================================================\n",
      "           Training Time (s) Accuracy Precision  Recall F1-Score\n",
      "sigmoid                 7.67   0.7694    0.7781  0.7694   0.7411\n",
      "tanh                    6.75   0.9694    0.9704  0.9694   0.9695\n",
      "relu                    4.96   0.9639    0.9655  0.9639   0.9642\n",
      "leaky_relu              5.34   0.9639    0.9655  0.9639   0.9642\n",
      "\n",
      "================================================================================\n",
      "FINAL ANALYSIS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Key Insights:\n",
      "1. No architecture achieves 100% accuracy - real data has inherent noise\n",
      "2. Deeper networks don't always perform better - can overfit on small datasets\n",
      "3. ReLU generally outperforms sigmoid/tanh for deeper networks\n",
      "4. Training time increases significantly with network depth\n",
      "5. Model complexity (parameters) doesn't guarantee better performance\n",
      "6. Validation is crucial to detect overfitting\n",
      "7. Different activation functions have different convergence characteristics\n",
      "\n",
      "Technical Observations:\n",
      "- Sigmoid/tanh suffer from vanishing gradients in deep networks\n",
      "- ReLU can have 'dying neuron' problem but generally trains faster\n",
      "- Leaky ReLU helps with dying neuron problem\n",
      "- Batch normalization and proper initialization are crucial\n",
      "- Learning rate scheduling can improve convergence\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer, load_digits, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class ActivationFunctions:\n",
    "    \"\"\"Collection of activation functions and their derivatives.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        # Clip to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        \"\"\"Derivative of sigmoid function.\"\"\"\n",
    "        s = ActivationFunctions.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        \"\"\"Hyperbolic tangent activation function.\"\"\"\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        \"\"\"Derivative of tanh function.\"\"\"\n",
    "        return 1 - np.tanh(z) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        \"\"\"ReLU activation function.\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        \"\"\"Derivative of ReLU function.\"\"\"\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaky_relu(z, alpha=0.01):\n",
    "        \"\"\"Leaky ReLU activation function.\"\"\"\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaky_relu_derivative(z, alpha=0.01):\n",
    "        \"\"\"Derivative of Leaky ReLU function.\"\"\"\n",
    "        return np.where(z > 0, 1, alpha)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        \"\"\"Softmax activation function for multi-class classification.\"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "class LossFunctions:\n",
    "    \"\"\"Collection of loss functions and their derivatives.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_squared_error(y_true, y_pred):\n",
    "        \"\"\"Mean Squared Error loss function.\"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse_derivative(y_true, y_pred):\n",
    "        \"\"\"Derivative of MSE loss function.\"\"\"\n",
    "        return 2 * (y_pred - y_true) / len(y_true)\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_cross_entropy(y_true, y_pred):\n",
    "        \"\"\"Binary Cross-Entropy loss function.\"\"\"\n",
    "        # Add small epsilon to prevent log(0)\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    @staticmethod\n",
    "    def bce_derivative(y_true, y_pred):\n",
    "        \"\"\"Derivative of Binary Cross-Entropy loss function.\"\"\"\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return (y_pred - y_true) / (y_pred * (1 - y_pred)) / len(y_true)\n",
    "    \n",
    "    @staticmethod\n",
    "    def categorical_cross_entropy(y_true, y_pred):\n",
    "        \"\"\"Categorical Cross-Entropy loss function.\"\"\"\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Flexible Neural Network implementation supporting different architectures.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers, activation='relu', output_activation='sigmoid', \n",
    "                 loss='binary_crossentropy', learning_rate=0.001, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        layers: list of integers specifying number of neurons in each layer\n",
    "        activation: activation function for hidden layers\n",
    "        output_activation: activation function for output layer\n",
    "        loss: loss function to use\n",
    "        learning_rate: learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.output_activation = output_activation\n",
    "        self.loss_function = loss\n",
    "        \n",
    "        # Initialize weights and biases using Xavier initialization\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(self.n_layers - 1):\n",
    "            # Xavier initialization\n",
    "            fan_in = layers[i]\n",
    "            fan_out = layers[i + 1]\n",
    "            limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "            \n",
    "            w = np.random.uniform(-limit, limit, (layers[i], layers[i + 1]))\n",
    "            b = np.zeros((1, layers[i + 1]))\n",
    "            \n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'loss': [],\n",
    "            'accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': []\n",
    "        }\n",
    "    \n",
    "    def _get_activation_function(self, name):\n",
    "        \"\"\"Get activation function by name.\"\"\"\n",
    "        activation_map = {\n",
    "            'sigmoid': (ActivationFunctions.sigmoid, ActivationFunctions.sigmoid_derivative),\n",
    "            'tanh': (ActivationFunctions.tanh, ActivationFunctions.tanh_derivative),\n",
    "            'relu': (ActivationFunctions.relu, ActivationFunctions.relu_derivative),\n",
    "            'leaky_relu': (ActivationFunctions.leaky_relu, ActivationFunctions.leaky_relu_derivative),\n",
    "            'softmax': (ActivationFunctions.softmax, None)\n",
    "        }\n",
    "        return activation_map[name]\n",
    "    \n",
    "    def _forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network.\n",
    "        \n",
    "        Returns:\n",
    "        activations: list of activations for each layer\n",
    "        z_values: list of pre-activation values for each layer\n",
    "        \"\"\"\n",
    "        activations = [X]\n",
    "        z_values = []\n",
    "        \n",
    "        current_input = X\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(self.n_layers - 2):\n",
    "            z = np.dot(current_input, self.weights[i]) + self.biases[i]\n",
    "            z_values.append(z)\n",
    "            \n",
    "            activation_func, _ = self._get_activation_function(self.activation)\n",
    "            a = activation_func(z)\n",
    "            activations.append(a)\n",
    "            current_input = a\n",
    "        \n",
    "        # Output layer\n",
    "        z_output = np.dot(current_input, self.weights[-1]) + self.biases[-1]\n",
    "        z_values.append(z_output)\n",
    "        \n",
    "        output_activation_func, _ = self._get_activation_function(self.output_activation)\n",
    "        a_output = output_activation_func(z_output)\n",
    "        activations.append(a_output)\n",
    "        \n",
    "        return activations, z_values\n",
    "    \n",
    "    def _backward_propagation(self, X, y, activations, z_values):\n",
    "        \"\"\"\n",
    "        Backward propagation to compute gradients.\n",
    "        \n",
    "        Returns:\n",
    "        weight_gradients: list of weight gradients\n",
    "        bias_gradients: list of bias gradients\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # number of samples\n",
    "        \n",
    "        weight_gradients = []\n",
    "        bias_gradients = []\n",
    "        \n",
    "        # Calculate output layer error\n",
    "        if self.loss_function == 'binary_crossentropy' and self.output_activation == 'sigmoid':\n",
    "            # Special case: BCE + Sigmoid has simplified derivative\n",
    "            delta = activations[-1] - y\n",
    "        else:\n",
    "            # General case\n",
    "            loss_derivative = LossFunctions.bce_derivative(y, activations[-1])\n",
    "            if self.output_activation != 'softmax':\n",
    "                _, activation_derivative = self._get_activation_function(self.output_activation)\n",
    "                delta = loss_derivative * activation_derivative(z_values[-1])\n",
    "            else:\n",
    "                delta = activations[-1] - y  # Softmax + CCE simplification\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dW = np.dot(activations[-2].T, delta) / m\n",
    "        db = np.mean(delta, axis=0, keepdims=True)\n",
    "        weight_gradients.append(dW)\n",
    "        bias_gradients.append(db)\n",
    "        \n",
    "        # Hidden layers (backward)\n",
    "        for i in range(self.n_layers - 2, 0, -1):\n",
    "            _, activation_derivative = self._get_activation_function(self.activation)\n",
    "            delta = np.dot(delta, self.weights[i].T) * activation_derivative(z_values[i-1])\n",
    "            \n",
    "            dW = np.dot(activations[i-1].T, delta) / m\n",
    "            db = np.mean(delta, axis=0, keepdims=True)\n",
    "            \n",
    "            weight_gradients.append(dW)\n",
    "            bias_gradients.append(db)\n",
    "        \n",
    "        # Reverse to match layer order\n",
    "        weight_gradients.reverse()\n",
    "        bias_gradients.reverse()\n",
    "        \n",
    "        return weight_gradients, bias_gradients\n",
    "    \n",
    "    def _update_parameters(self, weight_gradients, bias_gradients):\n",
    "        \"\"\"Update weights and biases using gradients.\"\"\"\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * weight_gradients[i]\n",
    "            self.biases[i] -= self.learning_rate * bias_gradients[i]\n",
    "    \n",
    "    def _compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute loss based on specified loss function.\"\"\"\n",
    "        if self.loss_function == 'binary_crossentropy':\n",
    "            return LossFunctions.binary_cross_entropy(y_true, y_pred)\n",
    "        elif self.loss_function == 'categorical_crossentropy':\n",
    "            return LossFunctions.categorical_cross_entropy(y_true, y_pred)\n",
    "        elif self.loss_function == 'mse':\n",
    "            return LossFunctions.mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None, epochs=1000, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        X_train: training features\n",
    "        y_train: training labels\n",
    "        X_val: validation features (optional)\n",
    "        y_val: validation labels (optional)\n",
    "        epochs: number of training epochs\n",
    "        batch_size: batch size for mini-batch gradient descent\n",
    "        verbose: whether to print training progress\n",
    "        \"\"\"\n",
    "        n_samples = X_train.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Mini-batch gradient descent\n",
    "            epoch_loss = 0\n",
    "            n_batches = 0\n",
    "            \n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "            \n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch_end = min(i + batch_size, n_samples)\n",
    "                X_batch = X_shuffled[i:batch_end]\n",
    "                y_batch = y_shuffled[i:batch_end]\n",
    "                \n",
    "                # Forward propagation\n",
    "                activations, z_values = self._forward_propagation(X_batch)\n",
    "                \n",
    "                # Compute loss\n",
    "                batch_loss = self._compute_loss(y_batch, activations[-1])\n",
    "                epoch_loss += batch_loss\n",
    "                n_batches += 1\n",
    "                \n",
    "                # Backward propagation\n",
    "                weight_gradients, bias_gradients = self._backward_propagation(\n",
    "                    X_batch, y_batch, activations, z_values\n",
    "                )\n",
    "                \n",
    "                # Update parameters\n",
    "                self._update_parameters(weight_gradients, bias_gradients)\n",
    "            \n",
    "            # Calculate epoch metrics\n",
    "            epoch_loss /= n_batches\n",
    "            train_predictions = self.predict(X_train)\n",
    "            # Convert one-hot back to class labels for accuracy calculation\n",
    "            # for multi-class classification, the training labels are one-hot encoded but the predictions are single class indices, \n",
    "            #so we need to convert the one-hot back to class indices for the accuracy calculation.\n",
    "            if len(y_train.shape) > 1 and y_train.shape[1] > 1:\n",
    "                y_train_labels = np.argmax(y_train, axis=1)\n",
    "                train_accuracy = accuracy_score(y_train_labels, train_predictions)\n",
    "            else:\n",
    "                train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "            \n",
    "            self.history['loss'].append(epoch_loss)\n",
    "            self.history['accuracy'].append(train_accuracy)\n",
    "            \n",
    "            # Validation metrics\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_predictions = self.predict_proba(X_val)\n",
    "                val_loss = self._compute_loss(y_val, val_predictions)\n",
    "                val_pred_labels = self.predict(X_val)\n",
    "                if len(y_val.shape) > 1 and y_val.shape[1] > 1:\n",
    "                    y_val_labels = np.argmax(y_val, axis=1)\n",
    "                    val_accuracy = accuracy_score(y_val_labels, val_pred_labels)\n",
    "                else:\n",
    "                    val_accuracy = accuracy_score(y_val, val_pred_labels)\n",
    "                \n",
    "                self.history['val_loss'].append(val_loss)\n",
    "                self.history['val_accuracy'].append(val_accuracy)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and epoch % 100 == 0:\n",
    "                if X_val is not None:\n",
    "                    print(f\"Epoch {epoch:4d}: Loss={epoch_loss:.4f}, Acc={train_accuracy:.4f}, \"\n",
    "                          f\"Val_Loss={val_loss:.4f}, Val_Acc={val_accuracy:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch:4d}: Loss={epoch_loss:.4f}, Accuracy={train_accuracy:.4f}\")\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        activations, _ = self._forward_propagation(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        if self.output_activation == 'sigmoid':\n",
    "            return (probabilities > 0.5).astype(int).flatten()\n",
    "        else:  # softmax\n",
    "            return np.argmax(probabilities, axis=1)\n",
    "\n",
    "def comprehensive_model_evaluation(model, X_test, y_test, class_names=None):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of neural network model.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Make predictions\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    prediction_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Prediction time: {prediction_time:.4f} seconds\")\n",
    "    if prediction_time > 0:\n",
    "        print(f\"Predictions per second: {len(X_test)/prediction_time:.0f}\")\n",
    "    else:\n",
    "        print(f\"Predictions per second: >1000000 (very fast)\")\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\nOverall Performance Metrics:\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Prediction confidence analysis\n",
    "    print(f\"\\nPrediction Confidence Analysis:\")\n",
    "    if len(y_proba.shape) == 1 or y_proba.shape[1] == 1:\n",
    "        # Binary classification\n",
    "        confidences = np.abs(y_proba.flatten() - 0.5) + 0.5\n",
    "    else:\n",
    "        # Multi-class classification\n",
    "        confidences = np.max(y_proba, axis=1)\n",
    "    \n",
    "    print(f\"Mean confidence: {np.mean(confidences):.4f}\")\n",
    "    print(f\"Std confidence:  {np.std(confidences):.4f}\")\n",
    "    print(f\"Min confidence:  {np.min(confidences):.4f}\")\n",
    "    print(f\"Max confidence:  {np.max(confidences):.4f}\")\n",
    "    \n",
    "    # Confidence-based accuracy analysis\n",
    "    high_conf_mask = confidences > 0.8\n",
    "    med_conf_mask = (confidences > 0.6) & (confidences <= 0.8)\n",
    "    low_conf_mask = confidences <= 0.6\n",
    "    \n",
    "    if np.any(high_conf_mask):\n",
    "        high_conf_acc = accuracy_score(y_test[high_conf_mask], y_pred[high_conf_mask])\n",
    "        print(f\"\\nHigh confidence (>0.8): {np.sum(high_conf_mask)} samples, accuracy: {high_conf_acc:.4f}\")\n",
    "    \n",
    "    if np.any(med_conf_mask):\n",
    "        med_conf_acc = accuracy_score(y_test[med_conf_mask], y_pred[med_conf_mask])\n",
    "        print(f\"Medium confidence (0.6-0.8): {np.sum(med_conf_mask)} samples, accuracy: {med_conf_acc:.4f}\")\n",
    "    \n",
    "    if np.any(low_conf_mask):\n",
    "        low_conf_acc = accuracy_score(y_test[low_conf_mask], y_pred[low_conf_mask])\n",
    "        print(f\"Low confidence (‚â§0.6): {np.sum(low_conf_mask)} samples, accuracy: {low_conf_acc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'prediction_time': prediction_time\n",
    "    }\n",
    "\n",
    "def compare_architectures():\n",
    "    \"\"\"\n",
    "    Compare different neural network architectures on real datasets.\n",
    "    \"\"\"\n",
    "    print(\"NEURAL NETWORK ARCHITECTURE COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load and prepare breast cancer dataset (binary classification)\n",
    "    print(\"\\nDataset: Breast Cancer Wisconsin (Binary Classification)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    cancer_data = load_breast_cancer()\n",
    "    X_cancer, y_cancer = cancer_data.data, cancer_data.target\n",
    "    \n",
    "    # Split and scale data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_val_scaled, X_test_scaled, y_val, y_test = train_test_split(\n",
    "        X_test_scaled, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train_scaled)}\")\n",
    "    print(f\"Validation samples: {len(X_val_scaled)}\")\n",
    "    print(f\"Test samples: {len(X_test_scaled)}\")\n",
    "    print(f\"Features: {X_train_scaled.shape[1]}\")\n",
    "    print(f\"Classes: {len(np.unique(y_cancer))} (Malignant: {np.sum(y_cancer==1)}, Benign: {np.sum(y_cancer==0)})\")\n",
    "    \n",
    "    # Define different architectures to compare\n",
    "    architectures = {\n",
    "        'Shallow Network': [X_train_scaled.shape[1], 16, 1],\n",
    "        'Deep Narrow': [X_train_scaled.shape[1], 32, 16, 8, 1],\n",
    "        'Wide Network': [X_train_scaled.shape[1], 128, 64, 1],\n",
    "        'Very Deep': [X_train_scaled.shape[1], 64, 32, 16, 8, 4, 1]\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, layers in architectures.items():\n",
    "        print(f\"\\n{'='*20} {name} {'='*20}\")\n",
    "        print(f\"Architecture: {' ‚Üí '.join(map(str, layers))}\")\n",
    "        \n",
    "        # Create and train model\n",
    "        model = NeuralNetwork(\n",
    "            layers=layers,\n",
    "            activation='relu',\n",
    "            output_activation='sigmoid',\n",
    "            loss='binary_crossentropy',\n",
    "            learning_rate=0.001,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model.fit(\n",
    "            X_train_scaled, y_train.reshape(-1, 1),\n",
    "            X_val_scaled, y_val.reshape(-1, 1),\n",
    "            epochs=500,\n",
    "            batch_size=32,\n",
    "            verbose=False\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate model\n",
    "        metrics = comprehensive_model_evaluation(\n",
    "            model, X_test_scaled, y_test,\n",
    "            class_names=['Benign', 'Malignant']\n",
    "        )\n",
    "        \n",
    "        metrics['training_time'] = training_time\n",
    "        metrics['parameters'] = sum(w.size for w in model.weights) + sum(b.size for b in model.biases)\n",
    "        results[name] = metrics\n",
    "        \n",
    "        print(f\"\\nTraining time: {training_time:.2f} seconds\")\n",
    "        print(f\"Model parameters: {metrics['parameters']:,}\")\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ARCHITECTURE COMPARISON SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        name: {\n",
    "            'Parameters': results[name]['parameters'],\n",
    "            'Training Time (s)': f\"{results[name]['training_time']:.2f}\",\n",
    "            'Prediction Time (s)': f\"{results[name]['prediction_time']:.4f}\",\n",
    "            'Accuracy': f\"{results[name]['accuracy']:.4f}\",\n",
    "            'Precision': f\"{results[name]['precision']:.4f}\",\n",
    "            'Recall': f\"{results[name]['recall']:.4f}\",\n",
    "            'F1-Score': f\"{results[name]['f1']:.4f}\"\n",
    "        }\n",
    "        for name in architectures.keys()\n",
    "    }).T\n",
    "    \n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Best model analysis\n",
    "    best_accuracy = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "    fastest_training = min(results.keys(), key=lambda x: results[x]['training_time'])\n",
    "    most_efficient = min(results.keys(), key=lambda x: results[x]['parameters'])\n",
    "    \n",
    "    print(f\"\\nKey Findings:\")\n",
    "    print(f\"Best Accuracy: {best_accuracy} ({results[best_accuracy]['accuracy']:.4f})\")\n",
    "    print(f\"Fastest Training: {fastest_training} ({results[fastest_training]['training_time']:.2f}s)\")\n",
    "    print(f\"Most Efficient: {most_efficient} ({results[most_efficient]['parameters']:,} parameters)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def activation_function_comparison():\n",
    "    \"\"\"\n",
    "    Compare different activation functions on the same dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ACTIVATION FUNCTION COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load digits dataset (multi-class classification)\n",
    "    digits_data = load_digits()\n",
    "    X_digits, y_digits = digits_data.data, digits_data.target\n",
    "    \n",
    "    print(f\"\\nDataset: Handwritten Digits (Multi-class Classification)\")\n",
    "    print(f\"Samples: {len(X_digits)}\")\n",
    "    print(f\"Features: {X_digits.shape[1]} (8x8 pixel values)\")\n",
    "    print(f\"Classes: {len(np.unique(y_digits))} (digits 0-9)\")\n",
    "    \n",
    "    # Split and scale data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_digits, y_digits, test_size=0.2, random_state=42, stratify=y_digits\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert to one-hot encoding for multi-class\n",
    "    n_classes = len(np.unique(y_digits))\n",
    "    y_train_onehot = np.eye(n_classes)[y_train]\n",
    "    y_test_onehot = np.eye(n_classes)[y_test]\n",
    "    \n",
    "    # Define activation functions to compare\n",
    "    activations = ['sigmoid', 'tanh', 'relu', 'leaky_relu']\n",
    "    \n",
    "    activation_results = {}\n",
    "    \n",
    "    for activation in activations:\n",
    "        print(f\"\\n{'='*20} {activation.upper()} {'='*20}\")\n",
    "        \n",
    "        # Create model\n",
    "        model = NeuralNetwork(\n",
    "            layers=[X_train_scaled.shape[1], 128, 64, n_classes],\n",
    "            activation=activation,\n",
    "            output_activation='softmax',\n",
    "            loss='categorical_crossentropy',\n",
    "            learning_rate=0.001,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        start_time = time.time()\n",
    "        model.fit(\n",
    "            X_train_scaled, y_train_onehot,\n",
    "            epochs=300,\n",
    "            batch_size=32,\n",
    "            verbose=False\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate model\n",
    "        metrics = comprehensive_model_evaluation(\n",
    "            model, X_test_scaled, y_test,\n",
    "            class_names=[f'Digit {i}' for i in range(10)]\n",
    "        )\n",
    "        \n",
    "        metrics['training_time'] = training_time\n",
    "        activation_results[activation] = metrics\n",
    "        \n",
    "        print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Comparison summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ACTIVATION FUNCTION COMPARISON SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    activation_df = pd.DataFrame({\n",
    "        activation: {\n",
    "            'Training Time (s)': f\"{activation_results[activation]['training_time']:.2f}\",\n",
    "            'Accuracy': f\"{activation_results[activation]['accuracy']:.4f}\",\n",
    "            'Precision': f\"{activation_results[activation]['precision']:.4f}\",\n",
    "            'Recall': f\"{activation_results[activation]['recall']:.4f}\",\n",
    "            'F1-Score': f\"{activation_results[activation]['f1']:.4f}\"\n",
    "        }\n",
    "        for activation in activations\n",
    "    }).T\n",
    "    \n",
    "    print(activation_df)\n",
    "    \n",
    "    return activation_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"COMPREHENSIVE NEURAL NETWORKS ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"This analysis compares different neural network architectures\")\n",
    "    print(\"and activation functions on real-world datasets with inherent\")\n",
    "    print(\"noise and class overlap, showing realistic performance metrics.\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Run architecture comparison\n",
    "    architecture_results = compare_architectures()\n",
    "    \n",
    "    # Run activation function comparison\n",
    "    activation_results = activation_function_comparison()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL ANALYSIS SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(\"\\nKey Insights:\")\n",
    "    print(\"1. No architecture achieves 100% accuracy - real data has inherent noise\")\n",
    "    print(\"2. Deeper networks don't always perform better - can overfit on small datasets\")\n",
    "    print(\"3. ReLU generally outperforms sigmoid/tanh for deeper networks\")\n",
    "    print(\"4. Training time increases significantly with network depth\")\n",
    "    print(\"5. Model complexity (parameters) doesn't guarantee better performance\")\n",
    "    print(\"6. Validation is crucial to detect overfitting\")\n",
    "    print(\"7. Different activation functions have different convergence characteristics\")\n",
    "    \n",
    "    print(f\"\\nTechnical Observations:\")\n",
    "    print(\"- Sigmoid/tanh suffer from vanishing gradients in deep networks\")\n",
    "    print(\"- ReLU can have 'dying neuron' problem but generally trains faster\")\n",
    "    print(\"- Leaky ReLU helps with dying neuron problem\")\n",
    "    print(\"- Batch normalization and proper initialization are crucial\")\n",
    "    print(\"- Learning rate scheduling can improve convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c656f6-8c02-4f28-8029-bcc74b04d92d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

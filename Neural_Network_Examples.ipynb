{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafb7cfa-67b0-4c94-97c5-b90e1dac3b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Comprehensive Neural Network Hyperparameter & Architecture Experiments\n",
    "\n",
    "**Objectives:** Evaluate the impact of various modeling choices on three datasets:\n",
    "- **Covertype** (classification)\n",
    "- **Diabetes** (regression)\n",
    "- **MNIST** (classification)\n",
    "\n",
    "**Axes of Exploration:**\n",
    "1. **Architecture**: Deep vs. Wide vs. Balanced\n",
    "2. **Batch Size**: 32, 64, 128, 256\n",
    "3. **Epochs**: 10, 20, 30\n",
    "4. **Learning Rate**: 0.01, 0.001, 0.0001\n",
    "5. **Dropout**: 0.0, 0.2, 0.5\n",
    "6. **Optimizer**: SGD, RMSprop, Adam\n",
    "7. **LR Schedule**: constant, exponential decay (0.9)\n",
    "8. **Activation**: ReLU, LeakyReLU, ELU\n",
    "9. **Normalization**: with/without BatchNorm\n",
    "10. **Regularization**: L2 (0, 0.001)\n",
    "11. **Data Augmentation**: MNIST shifts/rotations\n",
    "12. **Depth vs. Width Ablation**: constant param count\n",
    "13. **Training/Inference Time**\n",
    "\n",
    "Results captured: test metrics, runtime, and parameter counts.\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_covtype, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks, optimizers, regularizers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# %%\n",
    "# Data loaders\n",
    "\n",
    "def load_covertype(sample_frac=0.2, random_state=42):\n",
    "    X, y = fetch_covtype(return_X_y=True)\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    idx = rng.choice(len(y), int(len(y)*sample_frac), replace=False)\n",
    "    X, y = X[idx], y[idx] - 1\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    y_ohe = OneHotEncoder(sparse_output=False).fit_transform(y.reshape(-1,1))\n",
    "    return train_test_split(X, y_ohe, test_size=0.2, random_state=random_state)\n",
    "\n",
    "\n",
    "def load_diabetes_data(test_size=0.2, random_state=42):\n",
    "    X, y = load_diabetes(return_X_y=True)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\n",
    "def load_mnist_data(test_size=0.2, random_state=42):\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    X = np.concatenate([x_train, x_test]).reshape(-1,28*28) / 255.0\n",
    "    y = np.concatenate([y_train, y_test])\n",
    "    y_ohe = OneHotEncoder(sparse_output=False).fit_transform(y.reshape(-1,1))\n",
    "    return train_test_split(X, y_ohe, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# %%\n",
    "# Model builder\n",
    "\n",
    "def build_model(input_dim, output_dim, architecture, lr, dropout, activation='relu', use_bn=True, l2_strength=0.0):\n",
    "    reg = regularizers.l2(l2_strength)\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    if architecture == 'deep':\n",
    "        units = [128,64,32]\n",
    "    elif architecture == 'balanced':\n",
    "        units = [128,64]\n",
    "    else:\n",
    "        units = [256]\n",
    "    for u in units:\n",
    "        model.add(layers.Dense(u, activation=None, kernel_regularizer=reg))\n",
    "        if use_bn:\n",
    "            model.add(layers.BatchNormalization())\n",
    "        if activation == 'leaky':\n",
    "            model.add(layers.LeakyReLU())\n",
    "        elif activation == 'elu':\n",
    "            model.add(layers.ELU())\n",
    "        else:\n",
    "            model.add(layers.Activation('relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(layers.Dropout(dropout))\n",
    "    if output_dim > 1:\n",
    "        model.add(layers.Dense(output_dim, activation='softmax'))\n",
    "        loss = 'categorical_crossentropy'\n",
    "    else:\n",
    "        model.add(layers.Dense(1, activation='linear'))\n",
    "        loss = 'mse'\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=lr),\n",
    "        loss=loss,\n",
    "        metrics=['accuracy'] if output_dim>1 else []\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# %%\n",
    "# Experiment loop\n",
    "\n",
    "datasets = [\n",
    "    ('Covertype', load_covertype, True),\n",
    "    ('Diabetes', load_diabetes_data, False),\n",
    "    ('MNIST', load_mnist_data, True)\n",
    "]\n",
    "architectures = ['deep','wide','balanced']\n",
    "batch_sizes = [32, 64, 128]\n",
    "epochs_list = [10, 20]\n",
    "learning_rates = [0.01, 0.001]\n",
    "dropout_rates = [0.0, 0.2, 0.5]\n",
    "activations = ['relu','leaky','elu']\n",
    "use_bn_opts = [True, False]\n",
    "l2_strengths = [0.0, 0.001]\n",
    "optimizers_map = {'Adam':optimizers.Adam, 'SGD':optimizers.SGD, 'RMSprop':optimizers.RMSprop}\n",
    "results = []\n",
    "\n",
    "for name, loader, is_class in datasets:\n",
    "    X_train, X_test, y_train, y_test = loader()\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = y_train.shape[1] if is_class else 1\n",
    "    for arch in architectures:\n",
    "        for bs in batch_sizes:\n",
    "            for ep in epochs_list:\n",
    "                for lr in learning_rates:\n",
    "                    for do in dropout_rates:\n",
    "                        for act in activations:\n",
    "                            for use_bn in use_bn_opts:\n",
    "                                for l2 in l2_strengths:\n",
    "                                    for opt_name, opt_cls in optimizers_map.items():\n",
    "                                        model = build_model(input_dim, output_dim, arch, lr, do, activation=act, use_bn=use_bn, l2_strength=l2)\n",
    "                                        start = time.time()\n",
    "                                        hist = model.fit(\n",
    "                                            X_train, y_train,\n",
    "                                            epochs=ep, batch_size=bs,\n",
    "                                            validation_split=0.1,\n",
    "                                            callbacks=[callbacks.EarlyStopping(patience=3, restore_best_weights=True)],\n",
    "                                            verbose=0\n",
    "                                        )\n",
    "                                        duration = time.time() - start\n",
    "                                        if is_class:\n",
    "                                            preds = model.predict(X_test)\n",
    "                                            y_pred = preds.argmax(axis=1)\n",
    "                                            y_true = y_test.argmax(axis=1)\n",
    "                                            acc = accuracy_score(y_true, y_pred)\n",
    "                                            prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "                                            rec = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "                                            f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "                                            results.append({\n",
    "                                                'Dataset':name,'Arch':arch,'Batch':bs,'Epochs':ep,'LR':lr,\n",
    "                                                'Dropout':do,'Act':act,'BatchNorm':use_bn,'L2':l2,'Opt':opt_name,\n",
    "                                                'Accuracy':acc,'Precision':prec,'Recall':rec,'F1':f1,'Time_s':duration\n",
    "                                            })\n",
    "                                        else:\n",
    "                                            pred = model.predict(X_test).flatten()\n",
    "                                            mse = mean_squared_error(y_test, pred)\n",
    "                                            mae = mean_absolute_error(y_test, pred)\n",
    "                                            rmse = np.sqrt(mse)\n",
    "                                            results.append({\n",
    "                                                'Dataset':name,'Arch':arch,'Batch':bs,'Epochs':ep,'LR':lr,\n",
    "                                                'Dropout':do,'Act':act,'BatchNorm':use_bn,'L2':l2,'Opt':opt_name,\n",
    "                                                'MSE':mse,'MAE':mae,'RMSE':rmse,'Time_s':duration\n",
    "                                            })\n",
    "\n",
    "# %%\n",
    "# Compile results\n",
    "df = pd.DataFrame(results)\n",
    "print(df.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
